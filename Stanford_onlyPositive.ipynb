{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arusia/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='./kvret_dataset_public/'\n",
    "with open(path+'kvret_train_public.json') as f:\n",
    "    dat=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+'kvret_dev_public.json') as f:\n",
    "    valid_dat=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+'kvret_test_public.json') as f:\n",
    "    test_dat=json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dic = defaultdict(str)\n",
    "for d in dat:\n",
    "    for _d in d['dialogue']:\n",
    "        dic.update(_d['data'].get('slots',{}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "dic = set([])\n",
    "for d in dat:\n",
    "    for _d in d['dialogue']:\n",
    "        slots = _d['data'].get('slots')\n",
    "        if not slots: continue\n",
    "        dic.update([slots.get('distance')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dic = defaultdict(str)\n",
    "for d in dat:\n",
    "    for _d in d['dialogue']:\n",
    "        dic.update(_d.get('data',{}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "requested_slots = list(dic['requested'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['kb', 'task', 'uuid'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['scenario'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'thursday', 'monday', 'poi_type', 'saturday', 'time', 'date', 'address', 'location', 'agenda', 'traffic_info', 'poi', 'party', 'friday', 'sunday', 'event', 'room', 'distance', 'tuesday', 'today', 'wednesday'}\n"
     ]
    }
   ],
   "source": [
    "s=set([])\n",
    "for d in dat:\n",
    "    try:\n",
    "        keys=d['scenario']['kb']['column_names']\n",
    "    except:\n",
    "        continue\n",
    "    s.update(keys)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no traffic', 'heavy traffic', 'moderate traffic', 'road block nearby', 'car collision nearby'}\n"
     ]
    }
   ],
   "source": [
    "s=set([])\n",
    "for d in dat:\n",
    "    try:\n",
    "        keys=[t['traffic_info'] for t in d['scenario']['kb']['items'] ]\n",
    "    except:\n",
    "        continue\n",
    "    s.update(keys)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'location information', 'weekly forecast', 'calendar'}\n"
     ]
    }
   ],
   "source": [
    "s=set([])\n",
    "for d in dat:\n",
    "    try:\n",
    "        keys=[d['scenario']['kb']['kb_title']]\n",
    "    except:\n",
    "        continue\n",
    "    s.update(keys)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weather', 'navigate', 'schedule'}\n"
     ]
    }
   ],
   "source": [
    "s=set([])\n",
    "for d in dat:\n",
    "    try:\n",
    "        keys=[d['scenario']['task']['intent']]\n",
    "    except:\n",
    "        print(d)\n",
    "        continue\n",
    "    s.update(keys)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'set'>, {'location information': {'navigate'}, 'calendar': {'schedule'}, 'weekly forecast': {'weather'}})\n"
     ]
    }
   ],
   "source": [
    "s=defaultdict(set)\n",
    "for d in dat:\n",
    "    try:\n",
    "        s[d['scenario']['kb']['kb_title']].update([d['scenario']['task']['intent']])\n",
    "    except:\n",
    "        print(d)\n",
    "        continue\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'set'>, {'navigate': {'poi_type', 'address', 'traffic_info', 'poi', 'distance'}, 'schedule': {'time', 'date', 'agenda', 'party', 'room', 'event'}, 'weather': {'thursday', 'monday', 'saturday', 'location', 'sunday', 'friday', 'today', 'tuesday', 'wednesday'}})\n"
     ]
    }
   ],
   "source": [
    "s=defaultdict(set)\n",
    "for d in dat:\n",
    "    try:\n",
    "        s[d['scenario']['task']['intent']].update(d['scenario']['kb']['column_names'])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dialogue': [{'turn': 'driver', 'data': {'end_dialogue': False, 'utterance': 'check forecast for new york'}}, {'turn': 'assistant', 'data': {'end_dialogue': False, 'requested': {'weather_attribute': True, 'date': False, 'location': True}, 'slots': {'location': ' new york'}, 'utterance': 'The weather this week will be hail and rain on Thursday and Sunday, with the lowest temperature being 20F and the highest 90F, Monday will be the only hot day. '}}, {'turn': 'driver', 'data': {'end_dialogue': False, 'utterance': 'Will it be overcast on friday?'}}, {'turn': 'assistant', 'data': {'end_dialogue': False, 'requested': {'weather_attribute': True, 'date': True, 'location': False}, 'slots': {'weather_attribute': 'overcast', 'date': 'Friday'}, 'utterance': 'On Friday it will be cloudy on New York'}}, {'turn': 'driver', 'data': {'end_dialogue': False, 'utterance': 'Thank you!'}}, {'turn': 'assistant', 'data': {'end_dialogue': True, 'requested': {'weather_attribute': False, 'date': False, 'location': False}, 'slots': {}, 'utterance': \"You're welcome!\"}}], 'scenario': {'kb': {'items': [{'monday': 'stormy, low of 50F, high of 70F', 'tuesday': 'cloudy, low of 20F, high of 40F', 'friday': 'windy, low of 20F, high of 40F', 'wednesday': 'clear skies, low of 50F, high of 60F', 'thursday': 'stormy, low of 50F, high of 70F', 'sunday': 'hail, low of 30F, high of 50F', 'location': 'manhattan', 'saturday': 'snow, low of 60F, high of 70F', 'today': 'monday'}, {'monday': 'overcast, low of 50F, high of 70F', 'tuesday': 'stormy, low of 90F, high of 100F', 'friday': 'clear skies, low of 70F, high of 80F', 'wednesday': 'frost, low of 50F, high of 60F', 'thursday': 'snow, low of 80F, high of 90F', 'sunday': 'dry, low of 70F, high of 80F', 'location': 'camarillo', 'saturday': 'blizzard, low of 40F, high of 50F', 'today': 'monday'}, {'monday': 'dry, low of 50F, high of 70F', 'tuesday': 'warm, low of 80F, high of 90F', 'friday': 'cloudy, low of 70F, high of 80F', 'wednesday': 'hail, low of 20F, high of 40F', 'thursday': 'blizzard, low of 40F, high of 60F', 'sunday': 'stormy, low of 90F, high of 100F', 'location': 'menlo park', 'saturday': 'foggy, low of 50F, high of 60F', 'today': 'monday'}, {'monday': 'hot, low of 70F, high of 90F', 'tuesday': 'foggy, low of 70F, high of 90F', 'friday': 'cloudy, low of 70F, high of 90F', 'wednesday': 'dry, low of 40F, high of 50F', 'thursday': 'hail, low of 40F, high of 50F', 'sunday': 'rain, low of 20F, high of 30F', 'location': 'new york', 'saturday': 'clear skies, low of 70F, high of 90F', 'today': 'monday'}, {'monday': 'windy, low of 70F, high of 80F', 'tuesday': 'overcast, low of 40F, high of 50F', 'friday': 'hail, low of 60F, high of 70F', 'wednesday': 'hail, low of 90F, high of 100F', 'thursday': 'rain, low of 80F, high of 100F', 'sunday': 'frost, low of 20F, high of 30F', 'location': 'corona', 'saturday': 'warm, low of 60F, high of 80F', 'today': 'monday'}, {'monday': 'cloudy, low of 90F, high of 100F', 'tuesday': 'dry, low of 30F, high of 40F', 'friday': 'foggy, low of 20F, high of 30F', 'wednesday': 'stormy, low of 80F, high of 100F', 'thursday': 'snow, low of 20F, high of 30F', 'sunday': 'drizzle, low of 50F, high of 60F', 'location': 'seattle', 'saturday': 'drizzle, low of 40F, high of 60F', 'today': 'monday'}, {'monday': 'drizzle, low of 70F, high of 80F', 'tuesday': 'overcast, low of 40F, high of 60F', 'friday': 'foggy, low of 50F, high of 60F', 'wednesday': 'overcast, low of 60F, high of 80F', 'thursday': 'snow, low of 20F, high of 40F', 'sunday': 'cloudy, low of 30F, high of 40F', 'location': 'grand rapids', 'saturday': 'dry, low of 90F, high of 100F', 'today': 'monday'}], 'column_names': ['location', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'today'], 'kb_title': 'weekly forecast'}, 'task': {'intent': 'weather'}, 'uuid': '89cae425-6ff7-48b4-8b42-ec50b541dc63'}}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "defaultdict(<class 'set'>, {})\n"
     ]
    }
   ],
   "source": [
    "s=defaultdict(set)\n",
    "for d in dat:\n",
    "    try:\n",
    "        if d['scenario']['task']['intent'] == 'weather':\n",
    "            print(d)\n",
    "            print('\\n\\n\\n\\n')\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KB results with beilef states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'str'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defaultdict(lambda: 'str')['hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def sim(a, b):\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "\n",
    "def close(a,b):\n",
    "    return sim(a,b)>=0.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import dateutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_op(s1,op,s2):\n",
    "    if isinstance(s1, (int, float)):\n",
    "        num1 = s1\n",
    "    else:\n",
    "        m1=re.search(r'([0-9]+)',s1)\n",
    "        if m1 is None:\n",
    "            try:\n",
    "                num1=dateutil.parser.parse(s1)\n",
    "            except ValueError:\n",
    "                return False,None\n",
    "        else:\n",
    "          num1=int(m1.group())\n",
    "           \n",
    "    if isinstance(s2,(int,float)):\n",
    "        num2 = s2\n",
    "    else:\n",
    "        m2=re.search(r'([0-9]+)',s2)\n",
    "        if m2 is None:\n",
    "            try:\n",
    "                num2=dateutil.parser.parse(s2)\n",
    "            except ValueError:\n",
    "                return False ,None     \n",
    "        else:\n",
    "          num2=int(m2.group())\n",
    "    try:\n",
    "        if op=='equal to':\n",
    "            return num1==num2,num1\n",
    "        if op=='greater than':\n",
    "            return num1>num2,num1\n",
    "        if op=='less than':\n",
    "            return num1<num2,num1\n",
    "    except TypeError:\n",
    "        return False,num1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kb_results(kb_data,kb_intent,columns,pred_kb_intent,belief_state,operation):\n",
    "    \"\"\"This function gets the kb_data, column names and intent for which the kb is received. \n",
    "    If intent identified by the bot is nto same as kb_intent no results will be returned.\n",
    "    \n",
    "    intents types: {'schedule', 'weather', 'navigate'}\n",
    "    \n",
    "    operation should have same keys as belief state with following possible values:\n",
    "    str, =, >, <, minimum, maximum  indexed from 0 to 5\n",
    "    Use None for all the values not numerical. If not None, operation would be performend\n",
    "    \"\"\"\n",
    "    #defaultdict(<class 'set'>, {'navigate': {'poi', 'distance', 'poi_type', 'traffic_info', 'address'},, 'weather': {'thursday', 'sunday', 'today', 'friday', 'wednesday', 'tuesday', 'saturday', 'location', 'monday'}})\n",
    "    if pred_kb_intent!=kb_intent:\n",
    "        return [],[]\n",
    "    if kb_data is None:\n",
    "        return [],[]\n",
    "    results = np.ones(len(kb_data))\n",
    "    confidence = np.ones(len(kb_data))\n",
    "    # column names possiblity: {'room', 'party', 'event', 'agenda', 'date', 'time'}  \n",
    "    # Note that date and time are immutable and non-comparable in current dialog, so they are treated as strings\n",
    "    col_types = defaultdict(lambda: 'str')\n",
    "    if any(k not in columns for k in belief_state.keys()):\n",
    "        return [],[]\n",
    "    for k in belief_state.keys():\n",
    "        min_idx = None\n",
    "        min_val = float('Inf')\n",
    "        max_idx = None\n",
    "        max_val = -float('Inf')\n",
    "        for i,items in enumerate(kb_data):\n",
    "            if results[i] == 0: continue\n",
    "            if col_types[k]=='str':\n",
    "                if not (belief_state.get(k) and items.get(k) and operation.get(k) is not None):\n",
    "                    results[i]=0\n",
    "                    confidence[i]=0\n",
    "                else:\n",
    "                    results[i]=0\n",
    "                    if operation[k]==0 and close(belief_state[k],items[k]):\n",
    "                        # Doing string comparison\n",
    "                        results[i]=1\n",
    "                        confidence[i] = confidence[i]*sim(belief_state.get(k),items.get(k))\n",
    "                    elif operation[k]==1:\n",
    "                        #Doing equal comparison extracting the first number\n",
    "                        if num_op(belief_state[k],'equal to',items[k])[0]:\n",
    "                            results[i]=1\n",
    "                    elif operation[k]==2:\n",
    "                        #Doing greater than comparison extracting the first number\n",
    "                        if num_op(belief_state[k],'less than',items[k])[0]:\n",
    "                            results[i]=1\n",
    "                    elif operation[k]==3:\n",
    "                        #Doing less than comparison extracting the first number\n",
    "                        if num_op(belief_state[k],'greater than',items[k])[0]:\n",
    "                            results[i]=1\n",
    "                    elif operation[k]==4:\n",
    "                        #Doing mimum comparison extracting the first number\n",
    "                        res,val = num_op(items[k],'less than',(min_val))\n",
    "                        if res:\n",
    "                            results[i]=1\n",
    "                            if min_idx is not None:\n",
    "                                results[min_idx] = 0\n",
    "                                confidence[min_idx] = 0\n",
    "                            min_val = val\n",
    "                            min_idx = i\n",
    "                    elif operation[k]==5:\n",
    "                        #Doing maximum comparison extracting the first number\n",
    "                        res,val = num_op(items[k],'greater than',(max_val))\n",
    "                        if res:\n",
    "                            results[i]=1\n",
    "                            if max_idx is not None:\n",
    "                                results[max_idx] = 0\n",
    "                                confidence[max_idx] = 0\n",
    "                            max_val = val\n",
    "                            max_idx = i\n",
    "    \n",
    "           \n",
    "    return np.array(results),np.array(confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_data = dat[22]['scenario']['kb']['items']\n",
    "kb_intent = dat[22]['scenario']['task']['intent']\n",
    "columns = dat[22]['scenario']['kb']['column_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_kb_intent = kb_intent\n",
    "belief_state = {'traffic_info':'no traffic','distance':'6 miles'}\n",
    "operation = {'traffic_info':0,'distance':5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " array([0.        , 1.        , 0.        , 1.        , 0.69230769,\n",
       "        1.        , 0.69565217, 1.        ]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb_results(kb_data,kb_intent,columns,pred_kb_intent,belief_state,operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'address': '271 Springer Street',\n",
       " 'distance': '1 miles',\n",
       " 'poi': 'Mandarin Roots',\n",
       " 'poi_type': 'chinese restaurant',\n",
       " 'traffic_info': 'no traffic'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb_data[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(t):\n",
    "    return word_tokenize(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2425/2425 [00:00<00:00, 86080.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# collection of kb documents\n",
    "doc_kb=[]\n",
    "for d in tqdm(dat):\n",
    "    try:\n",
    "        for item in d['scenario']['kb']['items']:\n",
    "            [doc_kb.append(t)  for t in item.values()]\n",
    "    except TypeError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2425/2425 [00:00<00:00, 218265.82it/s]\n"
     ]
    }
   ],
   "source": [
    "doc_colnames=[]\n",
    "for d in tqdm(dat):\n",
    "    try:\n",
    "        item = d['scenario']['kb']['column_names']\n",
    "        [doc_colnames.append(t)  for t in item]\n",
    "    except TypeError:\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns=list(set(doc_colnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary for the databases\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer=Tokenizer(filters=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(doc_kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts([\"<SOS>\",\"<EOS>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_VOCAB_LEN = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also fitting in all the conversations and other things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_dict(d):\n",
    "    texts=[]\n",
    "    texts.append(\" \".join(list(d.keys())))\n",
    "    for v in d.values():\n",
    "        if isinstance(v,str):\n",
    "            texts.append(v) \n",
    "        elif isinstance(v,list):\n",
    "            texts.append(\" \".join(all_texts(v)))\n",
    "        elif isinstance(v,dict):\n",
    "            texts.append(\" \".join(all_dict(v)))\n",
    "        else:\n",
    "            try:\n",
    "                texts.append(str(v))\n",
    "            except:\n",
    "                raise Exception(f'type of v is {type(v)}')\n",
    "    return texts\n",
    "\n",
    "def all_texts(data):\n",
    "    texts = []\n",
    "    for d in data:\n",
    "        if isinstance(d,dict):\n",
    "            texts.append(\" \".join(all_dict(d)))\n",
    "        elif isinstance(d,list):\n",
    "            texts.append(\" \".join(all_texts(d)))\n",
    "        elif isinstance(d,str):\n",
    "            texts.append(d)\n",
    "        else:\n",
    "            try:\n",
    "                texts.append(str(d))\n",
    "            except:\n",
    "                raise Exception(f'type of d is {type(d)}')\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(all_texts(valid_dat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(all_texts(dat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(all_texts(test_dat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "625"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['ok']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the db model first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns_wi={'address': 5,\n",
    " 'agenda': 0,\n",
    " 'date': 6,\n",
    " 'distance': 8,\n",
    " 'event': 7,\n",
    " 'friday': 14,\n",
    " 'location': 2,\n",
    " 'monday': 10,\n",
    " 'party': 9,\n",
    " 'poi': 1,\n",
    " 'poi_type': 19,\n",
    " 'room': 15,\n",
    " 'saturday': 4,\n",
    " 'sunday': 12,\n",
    " 'thursday': 16,\n",
    " 'time': 17,\n",
    " 'today': 13,\n",
    " 'traffic_info': 11,\n",
    " 'tuesday': 3,\n",
    " 'wednesday': 18}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all_columns\n",
    "assert tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_QUERIES = 1\n",
    "NUM_COL = len(all_columns)\n",
    "CONV_VOCAB_LEN = len(tokenizer.word_index)+1\n",
    "THRESHOLD = 0.5\n",
    "MAX_DB_RESULTS = 5\n",
    "MAX_ENTITY_LENGTH = 10\n",
    "OPERATOR_LEN = 6\n",
    "NUM_INTENTS = 3\n",
    "EMBEDDING_SIZE=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.zeros((5,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting rules based db to desired output first\n",
    "def results_to_vector(bs_output,pred_intent,operation,kb_data,kb_intent):\n",
    "    assert bs_output.shape == (NUM_COL,MAX_ENTITY_LENGTH,CONV_VOCAB_LEN)\n",
    "    assert operation.shape == (NUM_COL,OPERATOR_LEN)\n",
    "    pred_intent = np.argmax(pred_intent) if max(pred_intent)>THRESHOLD else None\n",
    "    kb_intent = np.argmax(kb_intent)\n",
    "    output=np.zeros((MAX_DB_RESULTS,NUM_COL,MAX_ENTITY_LENGTH,CONV_VOCAB_LEN))\n",
    "    if intent is None:\n",
    "        return output\n",
    "    q=bs_output\n",
    "    op = operation\n",
    "    op_conf =  np.max(op,axis=-1)\n",
    "    op_classes = np.argmax(op,axis=-1) \n",
    "    op_classes = [_q if _q_conf>THRESHOLD else None for _q,_q_conf in zip(op_classes,op_conf)]\n",
    "\n",
    "    q_ents = np.argmax(q,axis=-1)\n",
    "    q_confs = np.max(q,axis=-1)\n",
    "    q_mask = np.array(q_confs>THRESHOLD,dtype='float32')\n",
    "    q_ents = q_mask*q_ents\n",
    "    q_words = [\" \".join([tokenizer.index_word[_q] for _q in __q if _q!=0]) for __q in q_ents]\n",
    "    # Now that q_words and op_classes are known\n",
    "    bs={}\n",
    "    operations = {}\n",
    "    for j,ent in enumerate(q_words):\n",
    "        if ent is None or ent==\"\": continue\n",
    "        bs[all_columns[j]]=ent\n",
    "        operations[all_columns[j]] = op_classes[j]\n",
    "    result,confidence = kb_results(kb_data,kb_intent,columns,pred_intent,bs,operations)\n",
    "    result=np.array(result)\n",
    "    confidence=np.array(confidence)\n",
    "    result = result[np.argsort(confidence)[-1::-1]]\n",
    "    confidence = confidence[np.argsort(confidence)[-1::-1]]\n",
    "    final_result=[kb_data[_i] for _i,(c,r) in enumerate(zip(confidence,result)) if c>=THRESHOLD and r==1]\n",
    "    confidence=[confidence[_i] for _i,(c,r) in enumerate(zip(confidence,result)) if c>=THRESHOLD and r==1]\n",
    "    kb_result = np.zeros((MAX_DB_RESULTS,NUM_COL,MAX_ENTITY_LENGTH,CONV_VOCAB_LEN))\n",
    "    for j,r in enumerate(final_result):\n",
    "        if j==MAX_DB_RESULTS: break\n",
    "        for k,v in r.items():\n",
    "            kb_result[j,all_columns_wi[k]] = to_categorical(pad_sequences(tokenizer.texts_to_sequences([v]),\n",
    "                                                            padding='post',truncating='post',maxlen=MAX_ENTITY_LENGTH)\\\n",
    "                                                             ,num_classes=CONV_VOCAB_LEN)*confidence[j]\n",
    "    output = kb_result\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time': 0,\n",
       " 'event': 1,\n",
       " 'party': 2,\n",
       " 'saturday': 3,\n",
       " 'room': 4,\n",
       " 'friday': 5,\n",
       " 'tuesday': 6,\n",
       " 'wednesday': 7,\n",
       " 'today': 8,\n",
       " 'agenda': 9,\n",
       " 'monday': 10,\n",
       " 'poi': 11,\n",
       " 'traffic_info': 12,\n",
       " 'thursday': 13,\n",
       " 'sunday': 14,\n",
       " 'address': 15,\n",
       " 'location': 16,\n",
       " 'date': 17,\n",
       " 'distance': 18,\n",
       " 'poi_type': 19}"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_columns_wi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_output=np.zeros((NUM_COL,MAX_ENTITY_LENGTH,CONV_VOCAB_LEN))\n",
    "intent=np.array([1,0,0])\n",
    "operation = np.zeros((NUM_COL,OPERATOR_LEN))\n",
    "bs_output[16,0:2]=to_categorical(tokenizer.texts_to_sequences(['6 miles'])[0],num_classes=CONV_VOCAB_LEN)\n",
    "operation[16,0]=1.0\n",
    "kb_data = dat[0]['scenario']['kb']['items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=results_to_vector(bs_output,intent,operation,kb_data,kb_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, False, False, False, False, False, False, False, False]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.max(r[0,2,i,1:])>0 for i in range((MAX_ENTITY_LENGTH))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input,Dense,LSTM,Embedding,TimeDistributed, RepeatVector, Concatenate,Reshape\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1, 20, 50)\n"
     ]
    }
   ],
   "source": [
    "# model for db\n",
    "bs_input = Input(shape=(MAX_QUERIES,NUM_COL,MAX_ENTITY_LENGTH,CONV_VOCAB_LEN))\n",
    "intent_input = Input(shape=(MAX_QUERIES,NUM_INTENTS,))\n",
    "operation_input = Input(shape=(MAX_QUERIES,NUM_COL,OPERATOR_LEN))\n",
    "\n",
    "bs_proc = TimeDistributed(TimeDistributed(TimeDistributed(Dense(10,activation='sigmoid'))))(bs_input)\n",
    "LSTM_bs_emb = TimeDistributed(TimeDistributed(LSTM(50,return_sequences=False,return_state=False)))(bs_proc)\n",
    "rep_intent_input = TimeDistributed(RepeatVector(NUM_COL))(intent_input)\n",
    "print(LSTM_bs_emb.shape)\n",
    "all_steps = Concatenate(axis=-1)([LSTM_bs_emb,operation_input,rep_intent_input])\n",
    "all_steps = Lambda(lambda x: tf.reshape(x,shape=(-1,MAX_QUERIES,NUM_COL*(50+OPERATOR_LEN+NUM_INTENTS))))(all_steps)\n",
    "encoder_lstm = Dense(50,activation='relu')(all_steps)\n",
    "encoder_lstm = TimeDistributed(RepeatVector(MAX_DB_RESULTS))(encoder_lstm)\n",
    "\n",
    "decoder_lstm1 = TimeDistributed(LSTM(50,return_sequences=True))(encoder_lstm)\n",
    "\n",
    "decoder_lstm1 = Dense(NUM_COL*50,activation='relu')(decoder_lstm1)\n",
    "decoder_lstm1 = Lambda(lambda x: tf.reshape(x,shape=(-1,MAX_QUERIES,MAX_DB_RESULTS,NUM_COL,50)))(decoder_lstm1)\n",
    "\n",
    "\n",
    "decoder_lstm2 = TimeDistributed(Lambda(lambda x: K.tile(K.expand_dims(x,axis=-2),[1,1,1,MAX_ENTITY_LENGTH,1])))(decoder_lstm1)\n",
    "decoder_lstm3 = TimeDistributed(TimeDistributed(TimeDistributed(LSTM(10,return_sequences=True))))(decoder_lstm2)\n",
    "\n",
    "out = TimeDistributed(TimeDistributed(TimeDistributed(TimeDistributed(Dense(CONV_VOCAB_LEN,activation='softmax')))))(decoder_lstm3)\n",
    "db_model = Model(inputs=[bs_input,intent_input,operation_input],outputs=[out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 1, 20, 10, 61 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_51 (TimeDistri (None, 1, 20, 10, 10 61840       input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (None, 1, 3)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_53 (TimeDistri (None, 1, 20, 50)    12200       time_distributed_51[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 1, 20, 6)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_54 (TimeDistri (None, 1, 20, 3)     0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1, 20, 59)    0           time_distributed_53[0][0]        \n",
      "                                                                 input_12[0][0]                   \n",
      "                                                                 time_distributed_54[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 1, 1180)      0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 1, 50)        59050       lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_55 (TimeDistri (None, 1, 5, 50)     0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_56 (TimeDistri (None, 1, 5, 50)     20200       time_distributed_55[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 1, 5, 1000)   51000       time_distributed_56[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 1, 5, 20, 50) 0           dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_57 (TimeDistri (None, 1, 5, 20, 10, 0           lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_60 (TimeDistri (None, 1, 5, 20, 10, 2440        time_distributed_57[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_64 (TimeDistri (None, 1, 5, 20, 10, 68013       time_distributed_60[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 274,743\n",
      "Trainable params: 274,743\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "db_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_model.compile(optimizer='adam',loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(w, t = 1.0):\n",
    "    e = np.exp(np.array(w) / t)\n",
    "    dist = e / np.sum(e)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent': 'navigate'}"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat[0]['scenario']['task']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "nav_cols=['distance','traffic_info','poi_type','address','poi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = ['schedule', 'weather', 'navigate']\n",
    "def input_generator(batch_size,data=dat):\n",
    "    batch_data1=[]\n",
    "    batch_data2=[]\n",
    "    batch_data3=[]\n",
    "    target=[]\n",
    "    random_dat = [data[i] for i in np.random.permutation(len(data))]\n",
    "    ij=0\n",
    "    while True:\n",
    "        ij+=1\n",
    "        for d in random_dat:\n",
    "            kb_intent = d['scenario']['task']['intent']\n",
    "            if kb_intent!='navigate': continue\n",
    "            kb_col_names = d['scenario']['kb']['column_names']\n",
    "            kb_data = d['scenario']['kb']['items']\n",
    "            true_vec_intent = np.zeros(NUM_INTENTS)\n",
    "            true_vec_intent[intents.index(kb_intent)]=1.0\n",
    "            pred_intent = np.array([0,0,1])#softmax(np.random.normal(size=NUM_INTENTS,loc=100,scale=5))\n",
    "            bs_input = np.zeros((NUM_COL,MAX_ENTITY_LENGTH,CONV_VOCAB_LEN))\n",
    "            operation = np.zeros((NUM_COL,OPERATOR_LEN))\n",
    "            num_cols_to_have = np.random.randint(NUM_COL)\n",
    "            num_ents_to_have = [np.random.randint(MAX_ENTITY_LENGTH) for _ in range(num_cols_to_have)]\n",
    "            for ii in range(num_cols_to_have):\n",
    "                col_idx = all_columns_wi[nav_cols[np.random.randint(5)]]\n",
    "                for j in range(num_ents_to_have[ii]):\n",
    "                    ix=(col_idx,MAX_ENTITY_LENGTH-j-1)\n",
    "                    bs_input[ix] = softmax(np.random.normal(size=CONV_VOCAB_LEN,loc=100,scale=5))\n",
    "                operation[col_idx] = softmax(np.random.normal(size=OPERATOR_LEN,loc=100,scale=5))\n",
    "\n",
    "            batch_data1.append([bs_input])\n",
    "            batch_data2.append([operation])\n",
    "            batch_data3.append([pred_intent])\n",
    "            target.append([results_to_vector(bs_input,pred_intent,operation,kb_data,true_vec_intent)])\n",
    "            \n",
    "            if len(batch_data1)==batch_size:\n",
    "                yield [np.array(batch_data1),np.array(batch_data3),np.array(batch_data2)],np.array(target)\n",
    "                batch_data1=[]\n",
    "                batch_data2=[]\n",
    "                batch_data3=[]\n",
    "                target=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1=next(input_generator(1,dat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "def memory_exhausted():\n",
    "    if memory_percent_available()<=10:\n",
    "        print(\"Memory Exhausted\")\n",
    "        exit()\n",
    "class memCall(keras.callbacks.Callback):\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        memory_exhausted()\n",
    "        \n",
    "def memory_percent_available():\n",
    "    return psutil.virtual_memory().available/psutil.virtual_memory().total*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint=keras.callbacks.ModelCheckpoint('./tmp.h5',save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-c85320d9ddb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0masd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'asd' is not defined"
     ]
    }
   ],
   "source": [
    "asd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size=4\n",
    "db_model.fit_generator(input_generator(batch_size),validation_data=input_generator(batch_size,valid_dat),steps_per_epoch=100,epochs=10,validation_steps=50,callbacks=[memCall(),checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing db_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_model.load_weights('db_model_op3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-377-bbe20a319033>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_dat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/miniconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m   1467\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1468\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1469\u001b[0;31m             verbose=verbose)\n\u001b[0m\u001b[1;32m   1470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/miniconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(model, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m    341\u001b[0m                                  \u001b[0;34m'or (x, y). Found: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                                  str(generator_output))\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0mouts_per_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[0;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size=1\n",
    "db_model.evaluate_generator(input_generator(batch_size,valid_dat),steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_data = dat[22]['scenario']['kb']['items']\n",
    "kb_intent = dat[22]['scenario']['task']['intent']\n",
    "columns = dat[22]['scenario']['kb']['column_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_kb_intent = kb_intent\n",
    "belief_state = {'traffic_info':'no traffic','distance':'6 miles'}\n",
    "operation = {'traffic_info':0,'distance':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'traffic_info': 'no traffic', 'distance': '6 miles'}"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "belief_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " array([0.69230769, 1.        , 1.        , 1.        , 0.69230769,\n",
       "        1.        , 0.69565217, 1.        ]))"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb_results(kb_data,kb_intent,columns,pred_kb_intent,belief_state,operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[65, 20]]"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['no traffic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'address': 5,\n",
       " 'agenda': 0,\n",
       " 'date': 6,\n",
       " 'distance': 8,\n",
       " 'event': 7,\n",
       " 'friday': 14,\n",
       " 'location': 2,\n",
       " 'monday': 10,\n",
       " 'party': 9,\n",
       " 'poi': 1,\n",
       " 'poi_type': 19,\n",
       " 'room': 15,\n",
       " 'saturday': 4,\n",
       " 'sunday': 12,\n",
       " 'thursday': 16,\n",
       " 'time': 17,\n",
       " 'today': 13,\n",
       " 'traffic_info': 11,\n",
       " 'tuesday': 3,\n",
       " 'wednesday': 18}"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_columns_wi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_output=np.zeros((NUM_COL,MAX_ENTITY_LENGTH,CONV_VOCAB_LEN))\n",
    "intent=np.array([0,0,1])\n",
    "operation = np.zeros((NUM_COL,OPERATOR_LEN))\n",
    "bs_output[8,0:2]=to_categorical(tokenizer.texts_to_sequences(['6 miles'])[0],num_classes=CONV_VOCAB_LEN)\n",
    "operation[8] = np.array([1,0,0,0,0,0])\n",
    "kb_data = dat[0]['scenario']['kb']['items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "act=results_to_vector(bs_output,intent,operation,kb_data,intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=db_model.predict([np.array([[bs_output]]),np.array([[intent]]),np.array([[operation]])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "sett=next(input_generator(batch_size=1))\n",
    "print(np.max(sett[1]))\n",
    "pred=db_model.predict(sett[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'distance': '2 miles',\n",
       "  'traffic_info': 'road block nearby',\n",
       "  'poi_type': 'parking garage',\n",
       "  'address': '550 Alester Ave',\n",
       "  'poi': 'Dish Parking'},\n",
       " {'distance': '6 miles',\n",
       "  'traffic_info': 'no traffic',\n",
       "  'poi_type': 'parking garage',\n",
       "  'address': '610 Amarillo Ave',\n",
       "  'poi': 'Stanford Oval Parking'},\n",
       " {'distance': '4 miles',\n",
       "  'traffic_info': 'car collision nearby',\n",
       "  'poi_type': 'grocery store',\n",
       "  'address': '409 Bollard St',\n",
       "  'poi': 'Willows Market'},\n",
       " {'distance': '2 miles',\n",
       "  'traffic_info': 'moderate traffic',\n",
       "  'poi_type': 'rest stop',\n",
       "  'address': '329 El Camino Real',\n",
       "  'poi': 'The Westin'},\n",
       " {'distance': '1 miles',\n",
       "  'traffic_info': 'heavy traffic',\n",
       "  'poi_type': 'friends house',\n",
       "  'address': '580 Van Ness Ave',\n",
       "  'poi': 'toms house'},\n",
       " {'distance': '4 miles',\n",
       "  'traffic_info': 'heavy traffic',\n",
       "  'poi_type': 'pizza restaurant',\n",
       "  'address': '915 Arbol Dr',\n",
       "  'poi': 'Pizza Chicago'},\n",
       " {'distance': '6 miles',\n",
       "  'traffic_info': 'car collision nearby',\n",
       "  'poi_type': 'gas station',\n",
       "  'address': '200 Alester Ave',\n",
       "  'poi': 'Valero'},\n",
       " {'distance': '2 miles',\n",
       "  'traffic_info': 'no traffic',\n",
       "  'poi_type': 'chinese restaurant',\n",
       "  'address': '271 Springer Street',\n",
       "  'poi': 'Mandarin Roots'}]"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([975, 975, 975, 975, 975, 975, 975, 975, 975, 975]), 0.00016177582)"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred[0][0][0][0],axis=-1),np.max(pred[0][0][0][0][0],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 20, 10, 6183)"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'traffic'"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.index_word[13+7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sett[1][0][0][0,8,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'28326c68-3fd8-4174-8514-572eb7eb41a9'"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.index_word[4284]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([94, 12,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(sett[1][0][0][0,8,:,:],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 20, 10, 6183)"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5163, 5163, 5163, 4284, 4284, 4284, 4284, 4284, 4284, 4284])"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred[0][0][0,8,:,:],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]),\n",
       " array([[6.85629959e-04, 1.19636359e-03, 1.93287979e-03, 2.90148426e-03,\n",
       "         4.06647893e-03, 5.35815954e-03, 6.69364585e-03, 7.99789187e-03,\n",
       "         9.21595562e-03, 1.03155775e-02],\n",
       "        [2.39286840e-01, 5.97366273e-01, 6.49502516e-01, 6.56392395e-01,\n",
       "         6.57324791e-01, 6.57450616e-01, 6.57468081e-01, 6.57470405e-01,\n",
       "         6.57471001e-01, 6.57470763e-01],\n",
       "        [2.40557149e-01, 5.97840309e-01, 6.49594367e-01, 6.56408787e-01,\n",
       "         6.57327950e-01, 6.57451212e-01, 6.57468081e-01, 6.57470405e-01,\n",
       "         6.57470763e-01, 6.57470763e-01],\n",
       "        [9.64874402e-04, 2.16078409e-03, 4.09074733e-03, 6.69032289e-03,\n",
       "         9.71729029e-03, 1.28728319e-02, 1.59049593e-02, 1.86496116e-02,\n",
       "         2.10266095e-02, 2.30175927e-02],\n",
       "        [1.16282527e-03, 3.15412180e-03, 7.05307629e-03, 1.30900806e-02,\n",
       "         2.07442995e-02, 2.90748160e-02, 3.72073129e-02, 4.45777103e-02,\n",
       "         5.09344339e-02, 5.62427975e-02],\n",
       "        [8.25073977e-04, 1.68179534e-03, 3.05662630e-03, 4.98943031e-03,\n",
       "         7.40438793e-03, 1.01343058e-02, 1.29799610e-02, 1.57643817e-02,\n",
       "         1.83607079e-02, 2.06955187e-02],\n",
       "        [7.60388677e-04, 1.52737577e-03, 2.83892709e-03, 4.78402618e-03,\n",
       "         7.26528978e-03, 1.00114141e-02, 1.27064437e-02, 1.51177021e-02,\n",
       "         1.71378460e-02, 1.87577643e-02],\n",
       "        [2.40395620e-01, 5.97788334e-01, 6.49584949e-01, 6.56407058e-01,\n",
       "         6.57327354e-01, 6.57451093e-01, 6.57468200e-01, 6.57470644e-01,\n",
       "         6.57470763e-01, 6.57470763e-01],\n",
       "        [6.19287428e-04, 9.69467568e-04, 1.41191354e-03, 1.93425745e-03,\n",
       "         2.51429249e-03, 3.12448083e-03, 3.73735372e-03, 4.32972889e-03,\n",
       "         4.88482090e-03, 5.39242849e-03],\n",
       "        [2.40494564e-01, 5.97810268e-01, 6.49587274e-01, 6.56407475e-01,\n",
       "         6.57327354e-01, 6.57451093e-01, 6.57468081e-01, 6.57470405e-01,\n",
       "         6.57470703e-01, 6.57470763e-01],\n",
       "        [8.80803971e-04, 1.86186098e-03, 3.42008239e-03, 5.51143195e-03,\n",
       "         7.92984664e-03, 1.04035372e-02, 1.27032055e-02, 1.46941394e-02,\n",
       "         1.63315516e-02, 1.76305156e-02],\n",
       "        [9.18419857e-04, 1.92553597e-03, 3.43220122e-03, 5.36778150e-03,\n",
       "         7.57139036e-03, 9.85086057e-03, 1.20362258e-02, 1.40095027e-02,\n",
       "         1.57099646e-02, 1.71235595e-02],\n",
       "        [1.17606902e-03, 3.06777447e-03, 6.37968956e-03, 1.08778886e-02,\n",
       "         1.58688165e-02, 2.06494629e-02, 2.47916076e-02, 2.81508788e-02,\n",
       "         3.07599120e-02, 3.27286720e-02],\n",
       "        [9.65150073e-04, 2.18192860e-03, 4.18078480e-03, 6.90617179e-03,\n",
       "         1.00917164e-02, 1.33960824e-02, 1.65313631e-02, 1.93177797e-02,\n",
       "         2.16784626e-02, 2.36094594e-02],\n",
       "        [8.31310346e-04, 1.55302230e-03, 2.44832435e-03, 3.41005437e-03,\n",
       "         4.35232464e-03, 5.22216596e-03, 5.99426543e-03, 6.66218624e-03,\n",
       "         7.23062456e-03, 7.70974671e-03],\n",
       "        [1.06672745e-03, 2.58758431e-03, 5.09685697e-03, 8.30869097e-03,\n",
       "         1.16281966e-02, 1.45493243e-02, 1.68499537e-02, 1.85344443e-02,\n",
       "         1.97096914e-02, 2.05014441e-02],\n",
       "        [8.26445874e-04, 1.65398244e-03, 2.94000143e-03, 4.70980676e-03,\n",
       "         6.90321345e-03, 9.39462706e-03, 1.20314397e-02, 1.46692917e-02,\n",
       "         1.71934739e-02, 1.95264369e-02],\n",
       "        [2.40476966e-01, 5.97800374e-01, 6.49585545e-01, 6.56407058e-01,\n",
       "         6.57327354e-01, 6.57451212e-01, 6.57468081e-01, 6.57470405e-01,\n",
       "         6.57470644e-01, 6.57470763e-01],\n",
       "        [1.31426693e-03, 3.58841429e-03, 7.60701904e-03, 1.31499786e-02,\n",
       "         1.94407757e-02, 2.55969837e-02, 3.09944171e-02, 3.53582501e-02,\n",
       "         3.86771783e-02, 4.10800837e-02],\n",
       "        [8.47987016e-04, 1.76614011e-03, 3.23947729e-03, 5.23981452e-03,\n",
       "         7.56730558e-03, 9.94475931e-03, 1.21363634e-02, 1.40063278e-02,\n",
       "         1.55138550e-02, 1.66801307e-02]], dtype=float32))"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred[0][0],axis=-1),np.max(pred[0][0][0],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [111, 104,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [334, 153,  68,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [130, 131,  76,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [305, 111,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [ 89,  12,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(act[0],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1=next(input_generator(1,dat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.0)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(a1[0][0][0][0][0][-1]),np.max(a1[0][0][0][0][0][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pympler import muppy\n",
    "from pympler import summary\n",
    "all_objects = muppy.get_objects()\n",
    "sum1 = summary.summarize(all_objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_memory_usage(batch_size, model):\n",
    "    import numpy as np\n",
    "    from keras import backend as K\n",
    "\n",
    "    shapes_mem_count = 0\n",
    "    for l in model.layers:\n",
    "        single_layer_mem = 1\n",
    "        for s in l.output_shape:\n",
    "            if s is None:\n",
    "                continue\n",
    "            single_layer_mem *= s\n",
    "        shapes_mem_count += single_layer_mem\n",
    "\n",
    "    trainable_count = np.sum([K.count_params(p) for p in set(model.trainable_weights)])\n",
    "    non_trainable_count = np.sum([K.count_params(p) for p in set(model.non_trainable_weights)])\n",
    "\n",
    "    total_memory = 4.0*batch_size*(shapes_mem_count + trainable_count + non_trainable_count)\n",
    "    gbytes = np.round(total_memory / (1024.0 ** 3), 3)\n",
    "    return gbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model_memory_usage(16,db_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pympler import asizeof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asizeof.asizeof(a1)/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asizeof.asizeof(db_model.layers[0])/1024/1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the complete model now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model would receive the belief states and give out the sentences in the coeherent form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 30\n",
    "LATENT_DIM = 50\n",
    "CONV_VOCAB_LEN = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine.network import Network\n",
    "def get_model(db_m):\n",
    "    frozen_db_m = Network(db_m.input,db_m.output,name='frozen_db_model')\n",
    "    \n",
    "    text_in = Input(shape=(MAX_SEQ_LEN,EMBEDDING_SIZE))\n",
    "    text_tok_in = Input(shape=(MAX_SEQ_LEN,CONV_VOCAB_LEN))\n",
    "    emb_state_in_h = Input(shape=(LATENT_DIM,))\n",
    "    emb_state_in_c = Input(shape=(LATENT_DIM,))\n",
    "    \n",
    "    encoding,emb_state_out_h,emb_state_out_c = LSTM(LATENT_DIM,\\\n",
    "                                                    return_state=True)(text_in,\\\n",
    "                                            initial_state=[emb_state_in_h,emb_state_in_c])\n",
    "    \n",
    "    \n",
    "    \n",
    "    db_decoder_1 = Lambda(lambda x: K.tile(K.expand_dims(x,axis=-2),[1,MAX_QUERIES,1]))(encoding)\n",
    "    db_decoder_2 = LSTM(NUM_COL*LATENT_DIM,return_sequences=True)(db_decoder_1)\n",
    "    db_decoder_2 = Lambda(lambda x: tf.reshape(x,[-1,MAX_QUERIES,NUM_COL,LATENT_DIM]))(db_decoder_2)\n",
    "    \n",
    "    db_decoder_3 = Lambda(lambda x: K.tile(K.expand_dims(x,axis=-2),[1,1,1,MAX_ENTITY_LENGTH,1]))(db_decoder_2)\n",
    "    db = TimeDistributed(TimeDistributed(LSTM(LATENT_DIM,return_sequences=True)))(db_decoder_3)\n",
    "    bs_out = Dense(MAX_SEQ_LEN,activation='softmax')(db)\n",
    "    text_tok_in_reshape = Lambda(lambda x: tf.reshape(x,[-1,1,1,1,MAX_SEQ_LEN,CONV_VOCAB_LEN]))(text_tok_in)\n",
    "    \n",
    "    bs_out = Lambda(lambda x: K.expand_dims(x))(bs_out)\n",
    "    print(bs_out.shape,text_tok_in_reshape.shape)\n",
    "    bs_out = Lambda(lambda x: tf.reduce_sum(x[0]*x[1],axis=-2))([bs_out,text_tok_in])\n",
    "    print(bs_out.shape)\n",
    "    op_out = Dense(LATENT_DIM,activation='relu')(db_decoder_2)\n",
    "    op_out = Dense(OPERATOR_LEN,activation='softmax')(op_out)\n",
    "    print(op_out.shape)\n",
    "    intent_out = Dense(LATENT_DIM,activation='relu')(encoding)\n",
    "    intent_out = Dense(NUM_INTENTS,activation='softmax')(intent_out)\n",
    "    intent_out = RepeatVector(MAX_QUERIES)(intent_out)\n",
    "    model_bs = Model(inputs=[text_in,emb_state_in_h,emb_state_in_c,text_tok_in],outputs=[bs_out,intent_out,op_out])\n",
    "    # DECODER\n",
    "    print(bs_out,intent_out,op_out)\n",
    "    decoder_input_db = frozen_db_m([bs_out,intent_out,op_out])\n",
    "    other_bs_input = encoding\n",
    "    \n",
    "    #Mdecoder_input_b is of length None,MAX_QUERIES,MAX_DB_RESULTS,NUM_COL,MAX_ENTITY_LENGTH,CONV_VOCAB_LEN\n",
    "    dbout_encoder_maxent = TimeDistributed(TimeDistributed(TimeDistributed(LSTM(LATENT_DIM))))(decoder_input_db)\n",
    "    dbout_encoder_ncol =  Lambda(lambda x: tf.reshape(x,[-1,MAX_QUERIES,MAX_DB_RESULTS,NUM_COL*LATENT_DIM]))(dbout_encoder_maxent)\n",
    "    dbout_encoder_ncol = Dense(LATENT_DIM,activation='relu')(dbout_encoder_ncol)\n",
    "    dbout_encoder_dbr = TimeDistributed(LSTM(LATENT_DIM))(dbout_encoder_ncol)\n",
    "    other_bs_input_rep = RepeatVector(MAX_QUERIES)(other_bs_input)\n",
    "    dbout_encoder = Concatenate(axis=-1)([dbout_encoder_dbr,other_bs_input_rep])\n",
    "    _,dbout_s,dbout_h = LSTM(LATENT_DIM,return_state=True)(dbout_encoder)\n",
    "    decoder_hidden_inputs = [dbout_s,dbout_h]\n",
    "    \n",
    "    decoder_inputs = Input(shape=(MAX_SEQ_LEN,EMBEDDING_SIZE))\n",
    "    print(decoder_inputs.shape,decoder_hidden_inputs[0].shape,decoder_hidden_inputs[1].shape)\n",
    "    decoder_LSTM = LSTM(LATENT_DIM,return_sequences=True,return_state=True)\n",
    "    decoder_outputs,_,_ = decoder_LSTM(decoder_inputs,initial_state=decoder_hidden_inputs)\n",
    "    decoder_outputs = Dense(CONV_VOCAB_LEN,activation='softmax')(decoder_outputs)\n",
    "    \n",
    "    full_model = Model(inputs=[text_in,emb_state_in_h,emb_state_in_c,text_tok_in,decoder_inputs],outputs=[decoder_outputs])\n",
    "    full_model.compile(loss='categorical_crossentropy',optimizer='adam')\n",
    "    return full_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model=get_model(db_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input((10,vocab_len))\n",
    "encoder = LSTM(50,return_state=True)\n",
    "out,out_h,out_c=encoder(inp)\n",
    "out = Dense(vocab_len,activation='softmax')(out)\n",
    "model_rnn_2=Model(inputs=[inp],outputs=out)\n",
    "\n",
    "out_3 = frozen_db_model(model_rnn_2.output)\n",
    "out_35 = RepeatVector(10)(out_3)\n",
    "decoder = LSTM(50,return_sequences=True,return_state=True)\n",
    "out_4,_,_= decoder(out_35,initial_state=[out_h,out_c])\n",
    "out_4 = Dense(vocab_len,activation='softmax')(out_4)\n",
    "model_4 = Model(inputs=[model_rnn_2.input],outputs=[model_rnn_2.output,out_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
