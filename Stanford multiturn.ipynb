{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/usr/lib/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./kvret_dataset_public/kvret_train_public.json') as f:\n",
    "    dat=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./kvret_dataset_public/kvret_dev_public.json') as f:\n",
    "    valid_dat=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dialogue', 'scenario'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['turn', 'data'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat[1]['dialogue'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'driver'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat[1]['dialogue'][0]['turn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'end_dialogue': False, 'utterance': 'Where is the nearest gas station?'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat[1]['dialogue'][0]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'end_dialogue': False,\n",
       " 'requested': {'distance': True,\n",
       "  'traffic_info': False,\n",
       "  'poi_type': True,\n",
       "  'address': True,\n",
       "  'poi': False},\n",
       " 'slots': {'distance': 'nearest', 'poi_type': 'shopping mall'},\n",
       " 'utterance': \"The nearest shopping mall is Stanford Shopping Center. It's located 3 miles away and there is no traffic to reach it\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat[12]['dialogue'][1]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kb': {'items': [{'monday': 'drizzle, low of 80F, high of 90F',\n",
       "    'tuesday': 'rain, low of 80F, high of 90F',\n",
       "    'friday': 'blizzard, low of 20F, high of 30F',\n",
       "    'wednesday': 'snow, low of 40F, high of 60F',\n",
       "    'thursday': 'cloudy, low of 50F, high of 70F',\n",
       "    'sunday': 'foggy, low of 90F, high of 100F',\n",
       "    'location': 'inglewood',\n",
       "    'saturday': 'blizzard, low of 30F, high of 40F',\n",
       "    'today': 'monday'},\n",
       "   {'monday': 'cloudy, low of 60F, high of 80F',\n",
       "    'tuesday': 'humid, low of 80F, high of 90F',\n",
       "    'friday': 'blizzard, low of 40F, high of 50F',\n",
       "    'wednesday': 'drizzle, low of 90F, high of 100F',\n",
       "    'thursday': 'clear skies, low of 60F, high of 70F',\n",
       "    'sunday': 'humid, low of 80F, high of 100F',\n",
       "    'location': 'san mateo',\n",
       "    'saturday': 'overcast, low of 90F, high of 100F',\n",
       "    'today': 'monday'},\n",
       "   {'monday': 'snow, low of 20F, high of 30F',\n",
       "    'tuesday': 'dry, low of 70F, high of 80F',\n",
       "    'friday': 'windy, low of 90F, high of 100F',\n",
       "    'wednesday': 'frost, low of 30F, high of 50F',\n",
       "    'thursday': 'drizzle, low of 60F, high of 80F',\n",
       "    'sunday': 'snow, low of 50F, high of 70F',\n",
       "    'location': 'mountain view',\n",
       "    'saturday': 'foggy, low of 60F, high of 80F',\n",
       "    'today': 'monday'},\n",
       "   {'monday': 'dew, low of 50F, high of 70F',\n",
       "    'tuesday': 'foggy, low of 70F, high of 90F',\n",
       "    'friday': 'frost, low of 30F, high of 40F',\n",
       "    'wednesday': 'stormy, low of 30F, high of 40F',\n",
       "    'thursday': 'cloudy, low of 30F, high of 50F',\n",
       "    'sunday': 'rain, low of 90F, high of 100F',\n",
       "    'location': 'danville',\n",
       "    'saturday': 'windy, low of 40F, high of 60F',\n",
       "    'today': 'monday'},\n",
       "   {'monday': 'windy, low of 90F, high of 100F',\n",
       "    'tuesday': 'rain, low of 70F, high of 80F',\n",
       "    'friday': 'drizzle, low of 60F, high of 80F',\n",
       "    'wednesday': 'foggy, low of 60F, high of 70F',\n",
       "    'thursday': 'windy, low of 50F, high of 60F',\n",
       "    'sunday': 'blizzard, low of 50F, high of 60F',\n",
       "    'location': 'alhambra',\n",
       "    'saturday': 'windy, low of 50F, high of 60F',\n",
       "    'today': 'monday'},\n",
       "   {'monday': 'drizzle, low of 20F, high of 30F',\n",
       "    'tuesday': 'snow, low of 80F, high of 90F',\n",
       "    'friday': 'cloudy, low of 50F, high of 60F',\n",
       "    'wednesday': 'cloudy, low of 90F, high of 100F',\n",
       "    'thursday': 'drizzle, low of 50F, high of 60F',\n",
       "    'sunday': 'misty, low of 80F, high of 100F',\n",
       "    'location': 'atherton',\n",
       "    'saturday': 'windy, low of 60F, high of 70F',\n",
       "    'today': 'monday'},\n",
       "   {'monday': 'drizzle, low of 40F, high of 50F',\n",
       "    'tuesday': 'raining, low of 50F, high of 60F',\n",
       "    'friday': 'rain, low of 80F, high of 100F',\n",
       "    'wednesday': 'dew, low of 70F, high of 90F',\n",
       "    'thursday': 'overcast, low of 70F, high of 80F',\n",
       "    'sunday': 'dry, low of 20F, high of 30F',\n",
       "    'location': 'menlo park',\n",
       "    'saturday': 'windy, low of 40F, high of 50F',\n",
       "    'today': 'monday'}],\n",
       "  'column_names': ['location',\n",
       "   'monday',\n",
       "   'tuesday',\n",
       "   'wednesday',\n",
       "   'thursday',\n",
       "   'friday',\n",
       "   'saturday',\n",
       "   'sunday',\n",
       "   'today'],\n",
       "  'kb_title': 'weekly forecast'},\n",
       " 'task': {'intent': 'weather'},\n",
       " 'uuid': 'f2637de0-21c4-45c7-ad98-a1e30057e9a6'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat[10]['scenario']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dic = defaultdict(str)\n",
    "for d in dat:\n",
    "    for _d in d['dialogue']:\n",
    "        dic.update(_d['data'].get('slots',{}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "belief_state_names = list(dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(str,\n",
       "            {'distance': 'best possible route',\n",
       "             'poi_type': 'coffee ot tea place',\n",
       "             'traffic_info': 'avoid all heavy traffic',\n",
       "             'poi': \"chef chu's\",\n",
       "             'date': '1st',\n",
       "             'party': 'Ana',\n",
       "             'event': 'Swimming activity',\n",
       "             'time': '2pm',\n",
       "             'location': 'Seattle',\n",
       "             'weather_attribute': '7 day forecast',\n",
       "             'room': 'conference room 100',\n",
       "             'address': '271 Springer Street',\n",
       "             'agenda': 'medicine'})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "dic = set([])\n",
    "for d in dat:\n",
    "    for _d in d['dialogue']:\n",
    "        slots = _d['data'].get('slots')\n",
    "        if not slots: continue\n",
    "        dic.update([slots.get('distance')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dic = defaultdict(str)\n",
    "for d in dat:\n",
    "    for _d in d['dialogue']:\n",
    "        dic.update(_d.get('data',{}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "requested_slots = list(dic['requested'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['kb', 'task', 'uuid'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['scenario'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'date', 'party', 'address', 'event', 'distance', 'monday', 'tuesday', 'wednesday', 'location', 'traffic_info', 'poi_type', 'time', 'thursday', 'room', 'friday', 'saturday', 'sunday', 'today', 'poi', 'agenda'}\n"
     ]
    }
   ],
   "source": [
    "s=set([])\n",
    "for d in dat:\n",
    "    try:\n",
    "        keys=d['scenario']['kb']['column_names']\n",
    "    except:\n",
    "        continue\n",
    "    s.update(keys)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'heavy traffic', 'road block nearby', 'no traffic', 'moderate traffic', 'car collision nearby'}\n"
     ]
    }
   ],
   "source": [
    "s=set([])\n",
    "for d in dat:\n",
    "    try:\n",
    "        keys=[t['traffic_info'] for t in d['scenario']['kb']['items'] ]\n",
    "    except:\n",
    "        continue\n",
    "    s.update(keys)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'location information', 'weekly forecast', 'calendar'}\n"
     ]
    }
   ],
   "source": [
    "s=set([])\n",
    "for d in dat:\n",
    "    try:\n",
    "        keys=[d['scenario']['kb']['kb_title']]\n",
    "    except:\n",
    "        continue\n",
    "    s.update(keys)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'schedule', 'weather', 'navigate'}\n"
     ]
    }
   ],
   "source": [
    "s=set([])\n",
    "for d in dat:\n",
    "    try:\n",
    "        keys=[d['scenario']['task']['intent']]\n",
    "    except:\n",
    "        print(d)\n",
    "        continue\n",
    "    s.update(keys)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'set'>, {'location information': {'navigate'}, 'calendar': {'schedule'}, 'weekly forecast': {'weather'}})\n"
     ]
    }
   ],
   "source": [
    "s=defaultdict(set)\n",
    "for d in dat:\n",
    "    try:\n",
    "        s[d['scenario']['kb']['kb_title']].update([d['scenario']['task']['intent']])\n",
    "    except:\n",
    "        print(d)\n",
    "        continue\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'set'>, {'navigate': {'address', 'distance', 'traffic_info', 'poi_type', 'poi'}, 'schedule': {'date', 'party', 'event', 'time', 'room', 'agenda'}, 'weather': {'tuesday', 'wednesday', 'monday', 'location', 'saturday', 'thursday', 'friday', 'sunday', 'today'}})\n"
     ]
    }
   ],
   "source": [
    "s=defaultdict(set)\n",
    "for d in dat:\n",
    "    try:\n",
    "        s[d['scenario']['task']['intent']].update(d['scenario']['kb']['column_names'])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dialogue': [{'turn': 'driver', 'data': {'end_dialogue': False, 'utterance': 'check forecast for new york'}}, {'turn': 'assistant', 'data': {'end_dialogue': False, 'requested': {'weather_attribute': True, 'date': False, 'location': True}, 'slots': {'location': ' new york'}, 'utterance': 'The weather this week will be hail and rain on Thursday and Sunday, with the lowest temperature being 20F and the highest 90F, Monday will be the only hot day. '}}, {'turn': 'driver', 'data': {'end_dialogue': False, 'utterance': 'Will it be overcast on friday?'}}, {'turn': 'assistant', 'data': {'end_dialogue': False, 'requested': {'weather_attribute': True, 'date': True, 'location': False}, 'slots': {'weather_attribute': 'overcast', 'date': 'Friday'}, 'utterance': 'On Friday it will be cloudy on New York'}}, {'turn': 'driver', 'data': {'end_dialogue': False, 'utterance': 'Thank you!'}}, {'turn': 'assistant', 'data': {'end_dialogue': True, 'requested': {'weather_attribute': False, 'date': False, 'location': False}, 'slots': {}, 'utterance': \"You're welcome!\"}}], 'scenario': {'kb': {'items': [{'monday': 'stormy, low of 50F, high of 70F', 'tuesday': 'cloudy, low of 20F, high of 40F', 'friday': 'windy, low of 20F, high of 40F', 'wednesday': 'clear skies, low of 50F, high of 60F', 'thursday': 'stormy, low of 50F, high of 70F', 'sunday': 'hail, low of 30F, high of 50F', 'location': 'manhattan', 'saturday': 'snow, low of 60F, high of 70F', 'today': 'monday'}, {'monday': 'overcast, low of 50F, high of 70F', 'tuesday': 'stormy, low of 90F, high of 100F', 'friday': 'clear skies, low of 70F, high of 80F', 'wednesday': 'frost, low of 50F, high of 60F', 'thursday': 'snow, low of 80F, high of 90F', 'sunday': 'dry, low of 70F, high of 80F', 'location': 'camarillo', 'saturday': 'blizzard, low of 40F, high of 50F', 'today': 'monday'}, {'monday': 'dry, low of 50F, high of 70F', 'tuesday': 'warm, low of 80F, high of 90F', 'friday': 'cloudy, low of 70F, high of 80F', 'wednesday': 'hail, low of 20F, high of 40F', 'thursday': 'blizzard, low of 40F, high of 60F', 'sunday': 'stormy, low of 90F, high of 100F', 'location': 'menlo park', 'saturday': 'foggy, low of 50F, high of 60F', 'today': 'monday'}, {'monday': 'hot, low of 70F, high of 90F', 'tuesday': 'foggy, low of 70F, high of 90F', 'friday': 'cloudy, low of 70F, high of 90F', 'wednesday': 'dry, low of 40F, high of 50F', 'thursday': 'hail, low of 40F, high of 50F', 'sunday': 'rain, low of 20F, high of 30F', 'location': 'new york', 'saturday': 'clear skies, low of 70F, high of 90F', 'today': 'monday'}, {'monday': 'windy, low of 70F, high of 80F', 'tuesday': 'overcast, low of 40F, high of 50F', 'friday': 'hail, low of 60F, high of 70F', 'wednesday': 'hail, low of 90F, high of 100F', 'thursday': 'rain, low of 80F, high of 100F', 'sunday': 'frost, low of 20F, high of 30F', 'location': 'corona', 'saturday': 'warm, low of 60F, high of 80F', 'today': 'monday'}, {'monday': 'cloudy, low of 90F, high of 100F', 'tuesday': 'dry, low of 30F, high of 40F', 'friday': 'foggy, low of 20F, high of 30F', 'wednesday': 'stormy, low of 80F, high of 100F', 'thursday': 'snow, low of 20F, high of 30F', 'sunday': 'drizzle, low of 50F, high of 60F', 'location': 'seattle', 'saturday': 'drizzle, low of 40F, high of 60F', 'today': 'monday'}, {'monday': 'drizzle, low of 70F, high of 80F', 'tuesday': 'overcast, low of 40F, high of 60F', 'friday': 'foggy, low of 50F, high of 60F', 'wednesday': 'overcast, low of 60F, high of 80F', 'thursday': 'snow, low of 20F, high of 40F', 'sunday': 'cloudy, low of 30F, high of 40F', 'location': 'grand rapids', 'saturday': 'dry, low of 90F, high of 100F', 'today': 'monday'}], 'column_names': ['location', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'today'], 'kb_title': 'weekly forecast'}, 'task': {'intent': 'weather'}, 'uuid': '89cae425-6ff7-48b4-8b42-ec50b541dc63'}}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "defaultdict(<class 'set'>, {})\n"
     ]
    }
   ],
   "source": [
    "s=defaultdict(set)\n",
    "for d in dat:\n",
    "    try:\n",
    "        if d['scenario']['task']['intent'] == 'weather':\n",
    "            print(d)\n",
    "            print('\\n\\n\\n\\n')\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KB results with beilef states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['distance',\n",
       " 'poi_type',\n",
       " 'traffic_info',\n",
       " 'poi',\n",
       " 'date',\n",
       " 'party',\n",
       " 'event',\n",
       " 'time',\n",
       " 'location',\n",
       " 'weather_attribute',\n",
       " 'room',\n",
       " 'address',\n",
       " 'agenda']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "belief_state_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'str'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defaultdict(lambda: 'str')['hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def sim(a, b):\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "\n",
    "def close(a,b):\n",
    "    return sim(a,b)>=0.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import dateutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_op(s1,op,s2):\n",
    "    if isinstance(s1, (int, float)):\n",
    "        num1 = s1\n",
    "    else:\n",
    "        m1=re.search(r'([0-9]+)',s1)\n",
    "        if m1 is None:\n",
    "            try:\n",
    "                num1=dateutil.parser.parse(s1)\n",
    "            except ValueError:\n",
    "                return False,None\n",
    "        num1=int(m1.group())\n",
    "           \n",
    "    if isinstance(s2,(int,float)):\n",
    "        num2 = s2\n",
    "    else:\n",
    "        m2=re.search(r'([0-9]+)',s2)\n",
    "        if m2 is None:\n",
    "            try:\n",
    "                num2=dateutil.parser.parse(s2)\n",
    "            except ValueError:\n",
    "                return False ,None     \n",
    "        num2=int(m2.group())\n",
    "    try:\n",
    "        if op=='equal to':\n",
    "            return num1==num2,num1\n",
    "        if op=='greater than':\n",
    "            return num1>num2,num1\n",
    "        if op=='less than':\n",
    "            return num1<num2,num1\n",
    "    except TypeError:\n",
    "        return False,num1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kb_results(kb_data,kb_intent,columns,pred_kb_intent,belief_state,operation):\n",
    "    \"\"\"This function gets the kb_data, column names and intent for which the kb is received. \n",
    "    If intent identified by the bot is nto same as kb_intent no results will be returned.\n",
    "    \n",
    "    intents types: {'schedule', 'weather', 'navigate'}\n",
    "    \n",
    "    operation should have same keys as belief state with following possible values:\n",
    "    str, =, >, <, minimum, maximum  indexed from 0 to 5\n",
    "    Use None for all the values not numerical. If not None, operation would be performend\n",
    "    \"\"\"\n",
    "    #defaultdict(<class 'set'>, {'navigate': {'poi', 'distance', 'poi_type', 'traffic_info', 'address'},, 'weather': {'thursday', 'sunday', 'today', 'friday', 'wednesday', 'tuesday', 'saturday', 'location', 'monday'}})\n",
    "    if pred_kb_intent!=kb_intent:\n",
    "        return [],[]\n",
    "    if kb_data is None:\n",
    "        return [],[]\n",
    "    results = np.ones(len(kb_data))\n",
    "    confidence = np.ones(len(kb_data))\n",
    "    # column names possiblity: {'room', 'party', 'event', 'agenda', 'date', 'time'}  \n",
    "    # Note that date and time are immutable and non-comparable in current dialog, so they are treated as strings\n",
    "    col_types = defaultdict(lambda: 'str')\n",
    "    if any(k not in columns for k in belief_state.keys()):\n",
    "        return [],[]\n",
    "    for k in belief_state.keys():\n",
    "        min_idx = None\n",
    "        min_val = float('Inf')\n",
    "        max_idx = None\n",
    "        max_val = -float('Inf')\n",
    "        for i,items in enumerate(kb_data):\n",
    "            if results[i] == 0: continue\n",
    "            if col_types[k]=='str':\n",
    "                if not (belief_state.get(k) and items.get(k) and operation.get(k) is not None):\n",
    "                    results[i]=0\n",
    "                    confidence[i]=0\n",
    "                else:\n",
    "                    results[i]=0\n",
    "                    if operation[k]==0 and close(belief_state[k],items[k]):\n",
    "                        # Doing string comparison\n",
    "                        results[i]=1\n",
    "                        confidence[i] = confidence[i]*sim(belief_state.get(k),items.get(k))\n",
    "                    elif operation[k]==1:\n",
    "                        #Doing equal comparison extracting the first number\n",
    "                        if num_op(belief_state[k],'equal to',items[k])[0]:\n",
    "                            results[i]=1\n",
    "                    elif operation[k]==2:\n",
    "                        #Doing greater than comparison extracting the first number\n",
    "                        if num_op(belief_state[k],'less than',items[k])[0]:\n",
    "                            results[i]=1\n",
    "                    elif operation[k]==3:\n",
    "                        #Doing less than comparison extracting the first number\n",
    "                        if num_op(belief_state[k],'greater than',items[k])[0]:\n",
    "                            results[i]=1\n",
    "                    elif operation[k]==4:\n",
    "                        #Doing mimum comparison extracting the first number\n",
    "                        res,val = num_op(items[k],'less than',(min_val))\n",
    "                        if res:\n",
    "                            results[i]=1\n",
    "                            if min_idx is not None:\n",
    "                                results[min_idx] = 0\n",
    "                                confidence[min_idx] = 0\n",
    "                            min_val = val\n",
    "                            min_idx = i\n",
    "                    elif operation[k]==5:\n",
    "                        #Doing maximum comparison extracting the first number\n",
    "                        res,val = num_op(items[k],'greater than',(max_val))\n",
    "                        if res:\n",
    "                            results[i]=1\n",
    "                            if max_idx is not None:\n",
    "                                results[max_idx] = 0\n",
    "                                confidence[max_idx] = 0\n",
    "                            max_val = val\n",
    "                            max_idx = i\n",
    "    \n",
    "           \n",
    "    return np.array(results),np.array(confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_data = dat[22]['scenario']['kb']['items']\n",
    "kb_intent = dat[22]['scenario']['task']['intent']\n",
    "columns = dat[22]['scenario']['kb']['column_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_kb_intent = kb_intent\n",
    "belief_state = {'traffic_info':'no traffic'}\n",
    "operation = {'traffic_info':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 0., 1., 0., 1., 1., 1., 0.]),\n",
       " array([0.69230769, 1.        , 1.        , 1.        , 0.69230769,\n",
       "        1.        , 0.69565217, 1.        ]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb_results(kb_data,kb_intent,columns,pred_kb_intent,belief_state,operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(t):\n",
    "    return word_tokenize(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2425/2425 [00:00<00:00, 105878.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# collection of kb documents\n",
    "doc_kb=[]\n",
    "for d in tqdm(dat):\n",
    "    try:\n",
    "        for item in d['scenario']['kb']['items']:\n",
    "            [doc_kb.append(t)  for t in item.values()]\n",
    "    except TypeError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2425/2425 [00:00<00:00, 302075.59it/s]\n"
     ]
    }
   ],
   "source": [
    "doc_colnames=[]\n",
    "for d in tqdm(dat):\n",
    "    try:\n",
    "        item = d['scenario']['kb']['column_names']\n",
    "        [doc_colnames.append(t)  for t in item]\n",
    "    except TypeError:\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns=list(set(doc_colnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary for the databases\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer=Tokenizer(filters=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(doc_kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts([\"<SOS>\",\"<EOS>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the db model first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns_wi={a:i for i,a in enumerate(all_columns)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all_columns\n",
    "assert tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_QUERIES = 1\n",
    "NUM_COL = len(all_columns)\n",
    "DB_VOCAB_LEN = len(tokenizer.word_index)+1\n",
    "THRESHOLD = 0.5\n",
    "MAX_DB_RESULTS = 5\n",
    "MAX_ENTITY_LENGTH = 10\n",
    "OPERATOR_LEN = 6\n",
    "NUM_INTENTS = 3\n",
    "EMBEDDING_SIZE=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.zeros((5,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting rules based db to desired output first\n",
    "def results_to_vector(bs_output,pred_intent,operation,kb_data,kb_intent):\n",
    "    assert bs_output.shape == (NUM_COL,MAX_ENTITY_LENGTH,DB_VOCAB_LEN)\n",
    "    assert operation.shape == (NUM_COL,OPERATOR_LEN)\n",
    "    pred_intent = np.argmax(pred_intent) if max(pred_intent)>THRESHOLD else None\n",
    "    kb_intent = np.argmax(kb_intent)\n",
    "    output=np.zeros((MAX_DB_RESULTS,NUM_COL,MAX_ENTITY_LENGTH,DB_VOCAB_LEN))\n",
    "    if intent is None:\n",
    "        return output\n",
    "    q=bs_output\n",
    "    op = operation\n",
    "    op_conf =  np.max(op,axis=-1)\n",
    "    op_classes = np.argmax(op,axis=-1) \n",
    "    op_classes = [_q if _q_conf>THRESHOLD else None for _q,_q_conf in zip(op_classes,op_conf)]\n",
    "\n",
    "    q_ents = np.argmax(q,axis=-1)\n",
    "    q_confs = np.max(q,axis=-1)\n",
    "    q_mask = np.array(q_confs>THRESHOLD,dtype='float32')\n",
    "    q_ents = q_mask*q_ents\n",
    "    q_words = [\" \".join([tokenizer.index_word[_q] for _q in __q if _q!=0]) for __q in q_ents]\n",
    "    # Now that q_words and op_classes are known\n",
    "    bs={}\n",
    "    operations = {}\n",
    "    for j,ent in enumerate(q_words):\n",
    "        if ent is None or ent==\"\": continue\n",
    "        bs[all_columns[j]]=ent\n",
    "        operations[all_columns[j]] = op_classes[j]\n",
    "    result,confidence = kb_results(kb_data,kb_intent,columns,pred_intent,bs,operations)\n",
    "    result=np.array(result)\n",
    "    confidence=np.array(confidence)\n",
    "    result = result[np.argsort(confidence)[-1::-1]]\n",
    "    confidence = confidence[np.argsort(confidence)[-1::-1]]\n",
    "    final_result=[kb_data[_i] for _i,(c,r) in enumerate(zip(confidence,result)) if c>=THRESHOLD and r==1]\n",
    "    confidence=[confidence[_i] for _i,(c,r) in enumerate(zip(confidence,result)) if c>=THRESHOLD and r==1]\n",
    "    kb_result = np.zeros((MAX_DB_RESULTS,NUM_COL,MAX_ENTITY_LENGTH,DB_VOCAB_LEN))\n",
    "    for j,r in enumerate(final_result):\n",
    "        if j==MAX_DB_RESULTS: break\n",
    "        for k,v in r.items():\n",
    "            kb_result[j,all_columns_wi[k]] = to_categorical(pad_sequences(tokenizer.texts_to_sequences([v]),maxlen=MAX_ENTITY_LENGTH)\\\n",
    "                                                             ,num_classes=DB_VOCAB_LEN)*confidence[j]\n",
    "\n",
    "    output = kb_result\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_output=np.zeros((NUM_COL,MAX_ENTITY_LENGTH,DB_VOCAB_LEN))\n",
    "intent=np.array([1,0,0])\n",
    "operation = np.zeros((NUM_COL,OPERATOR_LEN))\n",
    "bs_output[4,0:2]=to_categorical(tokenizer.texts_to_sequences(['no traffic'])[0],num_classes=DB_VOCAB_LEN)\n",
    "operation[4] = np.array([1,0,0,0,0,0])\n",
    "kb_data = dat[0]['scenario']['kb']['items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(results_to_vector(bs_output,intent,operation,kb_data,intent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_output=np.zeros((NUM_COL,MAX_ENTITY_LENGTH,DB_VOCAB_LEN))\n",
    "intent=np.array([1,0,0])\n",
    "operation = np.zeros((NUM_COL,OPERATOR_LEN))\n",
    "bs_output[19,0]=to_categorical(tokenizer.texts_to_sequences(['dish'])[0],num_classes=DB_VOCAB_LEN)\n",
    "operation[19,0]=1.0\n",
    "kb_data = dat[0]['scenario']['kb']['items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input,Dense,LSTM,Embedding,TimeDistributed, RepeatVector, Concatenate,Reshape\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1, 20, 50)\n"
     ]
    }
   ],
   "source": [
    "# model for db\n",
    "bs_input = Input(shape=(MAX_QUERIES,NUM_COL,MAX_ENTITY_LENGTH,DB_VOCAB_LEN))\n",
    "intent_input = Input(shape=(MAX_QUERIES,NUM_INTENTS,))\n",
    "operation_input = Input(shape=(MAX_QUERIES,NUM_COL,OPERATOR_LEN))\n",
    "\n",
    "LSTM_bs_emb = TimeDistributed(TimeDistributed(LSTM(50,return_sequences=False,return_state=False)))(bs_input)\n",
    "rep_intent_input = TimeDistributed(RepeatVector(NUM_COL))(intent_input)\n",
    "print(LSTM_bs_emb.shape)\n",
    "all_steps = Concatenate(axis=-1)([LSTM_bs_emb,operation_input,rep_intent_input])\n",
    "all_steps = Lambda(lambda x: tf.reshape(x,shape=(-1,MAX_QUERIES,NUM_COL*(50+OPERATOR_LEN+NUM_INTENTS))))(all_steps)\n",
    "encoder_lstm = Dense(50,activation='relu')(all_steps)\n",
    "encoder_lstm = TimeDistributed(RepeatVector(MAX_DB_RESULTS))(encoder_lstm)\n",
    "\n",
    "decoder_lstm1 = TimeDistributed(LSTM(50,return_sequences=True))(encoder_lstm)\n",
    "\n",
    "decoder_lstm1 = Dense(NUM_COL*50,activation='relu')(decoder_lstm1)\n",
    "decoder_lstm1 = Lambda(lambda x: tf.reshape(x,shape=(-1,MAX_QUERIES,MAX_DB_RESULTS,NUM_COL,50)))(decoder_lstm1)\n",
    "\n",
    "\n",
    "decoder_lstm2 = TimeDistributed(Lambda(lambda x: K.tile(K.expand_dims(x,axis=-2),[1,1,1,MAX_ENTITY_LENGTH,1])))(decoder_lstm1)\n",
    "decoder_lstm3 = TimeDistributed(TimeDistributed(TimeDistributed(LSTM(50,return_sequences=True))))(decoder_lstm2)\n",
    "\n",
    "out = Dense(DB_VOCAB_LEN,activation='softmax')(decoder_lstm3)\n",
    "db_model = Model(inputs=[bs_input,intent_input,operation_input],outputs=[out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1, 20, 10, 38 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1, 3)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 1, 20, 50)    88000       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1, 20, 6)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 1, 20, 3)     0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 20, 59)    0           time_distributed_2[0][0]         \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 1180)      0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 50)        59050       lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 1, 5, 50)     0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 1, 5, 50)     20200       time_distributed_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1, 5, 1000)   51000       time_distributed_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1, 5, 20, 50) 0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 1, 5, 20, 10, 0           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistrib (None, 1, 5, 20, 10, 20200       time_distributed_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1, 5, 20, 10, 19839       time_distributed_9[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 258,289\n",
      "Trainable params: 258,289\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "db_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_model.compile(optimizer='adam',loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(w, t = 1.0):\n",
    "    e = np.exp(np.array(w) / t)\n",
    "    dist = e / np.sum(e)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = ['schedule', 'weather', 'navigate']\n",
    "def input_generator(batch_size,data=dat):\n",
    "    batch_data1=[]\n",
    "    batch_data2=[]\n",
    "    batch_data3=[]\n",
    "    target=[]\n",
    "    random_dat = [data[i] for i in np.random.permutation(len(data))]\n",
    "    ij=0\n",
    "    while True:\n",
    "        ij+=1\n",
    "        for d in random_dat:\n",
    "            kb_intent = d['scenario']['task']['intent']\n",
    "            kb_col_names = d['scenario']['kb']['column_names']\n",
    "            kb_data = d['scenario']['kb']['items']\n",
    "            true_vec_intent = np.zeros(NUM_INTENTS)\n",
    "            true_vec_intent[intents.index(kb_intent)]=1.0\n",
    "            pred_intent = softmax(np.random.normal(size=NUM_INTENTS,loc=100,scale=5))\n",
    "            bs_input = np.zeros((NUM_COL,MAX_ENTITY_LENGTH,DB_VOCAB_LEN))\n",
    "            operation = np.zeros((NUM_COL,OPERATOR_LEN))\n",
    "            num_cols_to_have = np.random.randint(NUM_COL)\n",
    "            num_ents_to_have = [np.random.randint(NUM_COL) for _ in range(num_cols_to_have)]\n",
    "            for ii in range(num_cols_to_have):\n",
    "                for j in range(num_ents_to_have[ii]):\n",
    "                    ix=(np.random.randint(NUM_COL),np.random.randint(MAX_ENTITY_LENGTH))\n",
    "                    bs_input[ix] = softmax(np.random.normal(size=DB_VOCAB_LEN,loc=100,scale=5))\n",
    "                operation[np.random.randint(NUM_COL)] = softmax(np.random.normal(size=OPERATOR_LEN,loc=100,scale=5))\n",
    "            batch_data1.append(np.array([bs_input]))\n",
    "            batch_data2.append(np.array([operation]))\n",
    "            batch_data3.append(np.array([pred_intent]))\n",
    "            target.append(np.array([results_to_vector(bs_input,pred_intent,operation,kb_data,true_vec_intent)]))\n",
    "            \n",
    "            if len(batch_data1)==batch_size:\n",
    "                yield [np.array(batch_data1),np.array(batch_data3),np.array(batch_data2)],np.array(target)\n",
    "                batch_data1=[]\n",
    "                batch_data2=[]\n",
    "                batch_data3=[]\n",
    "                target=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 20, 10, 389)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(input_generator(1,dat))[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "def memory_exhausted():\n",
    "    if memory_percent_available()<=10:\n",
    "        print(\"Memory Exhausted\")\n",
    "        exit()\n",
    "class memCall(keras.callbacks.Callback):\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        memory_exhausted()\n",
    "        \n",
    "def memory_percent_available():\n",
    "    return psutil.virtual_memory().available/psutil.virtual_memory().total*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint=keras.callbacks.ModelCheckpoint('./db_model.h5',save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 651s 521ms/step - loss: 0.0103 - val_loss: 0.0097\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 651s 521ms/step - loss: 0.0088 - val_loss: 0.0070\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 657s 526ms/step - loss: 0.0070 - val_loss: 0.0062\n",
      "Epoch 4/10\n",
      " 151/1250 [==>...........................] - ETA: 7:51 - loss: 0.0062"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-75341f0bd2e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_dat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1250\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m650\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmemCall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/miniconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/miniconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    212\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size=16\n",
    "db_model.fit_generator(input_generator(batch_size),validation_data=input_generator(batch_size,valid_dat),steps_per_epoch=1250,epochs=10,validation_steps=650,callbacks=[memCall(),checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_model.load_weights('./db_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_data = dat[22]['scenario']['kb']['items']\n",
    "kb_intent = dat[22]['scenario']['task']['intent']\n",
    "columns = dat[22]['scenario']['kb']['column_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_kb_intent = kb_intent\n",
    "belief_state = {'traffic_info':'no traffic','distance':'6 miles'}\n",
    "operation = {'traffic_info':0,'distance':5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " array([0.        , 1.        , 0.        , 1.        , 0.69230769,\n",
       "        1.        , 0.69565217, 1.        ]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb_results(kb_data,kb_intent,columns,pred_kb_intent,belief_state,operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[39, 19]]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['no traffic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_output=np.zeros((NUM_COL,MAX_ENTITY_LENGTH,DB_VOCAB_LEN))\n",
    "intent=np.array([1,0,0])\n",
    "operation = np.zeros((NUM_COL,OPERATOR_LEN))\n",
    "bs_output[4,0:2]=to_categorical(tokenizer.texts_to_sequences(['no traffic'])[0],num_classes=DB_VOCAB_LEN)\n",
    "operation[4] = np.array([1,0,0,0,0,0])\n",
    "kb_data = dat[0]['scenario']['kb']['items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "act=results_to_vector(bs_output,intent,operation,kb_data,intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=db_model.predict([np.array([[bs_output]]),np.array([[intent]]),np.array([[operation]])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(act[0][0],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 2, 2, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 2, 2, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 2, 2, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 2, 2, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 2, 2, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 2, 2, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 2, 2, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred[0][0][2],axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the NLG model first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "389*20*10*5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model would receive the belief states and give out the sentences in the coeherent form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input((10,vocab_len))\n",
    "encoder = LSTM(50,return_state=True)\n",
    "out,out_h,out_c=encoder(inp)\n",
    "out = Dense(vocab_len,activation='softmax')(out)\n",
    "model_rnn_2=Model(inputs=[inp],outputs=out)\n",
    "\n",
    "out_3 = frozen_db_model(model_rnn_2.output)\n",
    "out_35 = RepeatVector(10)(out_3)\n",
    "decoder = LSTM(50,return_sequences=True,return_state=True)\n",
    "out_4,_,_= decoder(out_35,initial_state=[out_h,out_c])\n",
    "out_4 = Dense(vocab_len,activation='softmax')(out_4)\n",
    "model_4 = Model(inputs=[model_rnn_2.input],outputs=[model_rnn_2.output,out_4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
