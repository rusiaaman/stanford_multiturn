{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arusia/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./kvret_train_public.json') as f:\n",
    "    dat=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./kvret_dev_public.json') as f:\n",
    "    valid_dat=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dialogue', 'scenario'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['turn', 'data'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat[1]['dialogue'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'driver'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat[1]['dialogue'][0]['turn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'end_dialogue': False, 'utterance': 'Where is the nearest gas station?'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat[1]['dialogue'][0]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'end_dialogue': False,\n",
       " 'requested': {'address': True,\n",
       "  'distance': True,\n",
       "  'poi': False,\n",
       "  'poi_type': True,\n",
       "  'traffic_info': False},\n",
       " 'slots': {'distance': 'nearest', 'poi_type': 'shopping mall'},\n",
       " 'utterance': \"The nearest shopping mall is Stanford Shopping Center. It's located 3 miles away and there is no traffic to reach it\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat[12]['dialogue'][1]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kb': {'column_names': ['location',\n",
       "   'monday',\n",
       "   'tuesday',\n",
       "   'wednesday',\n",
       "   'thursday',\n",
       "   'friday',\n",
       "   'saturday',\n",
       "   'sunday',\n",
       "   'today'],\n",
       "  'items': [{'friday': 'blizzard, low of 20F, high of 30F',\n",
       "    'location': 'inglewood',\n",
       "    'monday': 'drizzle, low of 80F, high of 90F',\n",
       "    'saturday': 'blizzard, low of 30F, high of 40F',\n",
       "    'sunday': 'foggy, low of 90F, high of 100F',\n",
       "    'thursday': 'cloudy, low of 50F, high of 70F',\n",
       "    'today': 'monday',\n",
       "    'tuesday': 'rain, low of 80F, high of 90F',\n",
       "    'wednesday': 'snow, low of 40F, high of 60F'},\n",
       "   {'friday': 'blizzard, low of 40F, high of 50F',\n",
       "    'location': 'san mateo',\n",
       "    'monday': 'cloudy, low of 60F, high of 80F',\n",
       "    'saturday': 'overcast, low of 90F, high of 100F',\n",
       "    'sunday': 'humid, low of 80F, high of 100F',\n",
       "    'thursday': 'clear skies, low of 60F, high of 70F',\n",
       "    'today': 'monday',\n",
       "    'tuesday': 'humid, low of 80F, high of 90F',\n",
       "    'wednesday': 'drizzle, low of 90F, high of 100F'},\n",
       "   {'friday': 'windy, low of 90F, high of 100F',\n",
       "    'location': 'mountain view',\n",
       "    'monday': 'snow, low of 20F, high of 30F',\n",
       "    'saturday': 'foggy, low of 60F, high of 80F',\n",
       "    'sunday': 'snow, low of 50F, high of 70F',\n",
       "    'thursday': 'drizzle, low of 60F, high of 80F',\n",
       "    'today': 'monday',\n",
       "    'tuesday': 'dry, low of 70F, high of 80F',\n",
       "    'wednesday': 'frost, low of 30F, high of 50F'},\n",
       "   {'friday': 'frost, low of 30F, high of 40F',\n",
       "    'location': 'danville',\n",
       "    'monday': 'dew, low of 50F, high of 70F',\n",
       "    'saturday': 'windy, low of 40F, high of 60F',\n",
       "    'sunday': 'rain, low of 90F, high of 100F',\n",
       "    'thursday': 'cloudy, low of 30F, high of 50F',\n",
       "    'today': 'monday',\n",
       "    'tuesday': 'foggy, low of 70F, high of 90F',\n",
       "    'wednesday': 'stormy, low of 30F, high of 40F'},\n",
       "   {'friday': 'drizzle, low of 60F, high of 80F',\n",
       "    'location': 'alhambra',\n",
       "    'monday': 'windy, low of 90F, high of 100F',\n",
       "    'saturday': 'windy, low of 50F, high of 60F',\n",
       "    'sunday': 'blizzard, low of 50F, high of 60F',\n",
       "    'thursday': 'windy, low of 50F, high of 60F',\n",
       "    'today': 'monday',\n",
       "    'tuesday': 'rain, low of 70F, high of 80F',\n",
       "    'wednesday': 'foggy, low of 60F, high of 70F'},\n",
       "   {'friday': 'cloudy, low of 50F, high of 60F',\n",
       "    'location': 'atherton',\n",
       "    'monday': 'drizzle, low of 20F, high of 30F',\n",
       "    'saturday': 'windy, low of 60F, high of 70F',\n",
       "    'sunday': 'misty, low of 80F, high of 100F',\n",
       "    'thursday': 'drizzle, low of 50F, high of 60F',\n",
       "    'today': 'monday',\n",
       "    'tuesday': 'snow, low of 80F, high of 90F',\n",
       "    'wednesday': 'cloudy, low of 90F, high of 100F'},\n",
       "   {'friday': 'rain, low of 80F, high of 100F',\n",
       "    'location': 'menlo park',\n",
       "    'monday': 'drizzle, low of 40F, high of 50F',\n",
       "    'saturday': 'windy, low of 40F, high of 50F',\n",
       "    'sunday': 'dry, low of 20F, high of 30F',\n",
       "    'thursday': 'overcast, low of 70F, high of 80F',\n",
       "    'today': 'monday',\n",
       "    'tuesday': 'raining, low of 50F, high of 60F',\n",
       "    'wednesday': 'dew, low of 70F, high of 90F'}],\n",
       "  'kb_title': 'weekly forecast'},\n",
       " 'task': {'intent': 'weather'},\n",
       " 'uuid': 'f2637de0-21c4-45c7-ad98-a1e30057e9a6'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat[10]['scenario']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dic = defaultdict(str)\n",
    "for d in dat:\n",
    "    for _d in d['dialogue']:\n",
    "        dic.update(_d['data'].get('slots',{}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "belief_state_names = list(dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(str,\n",
       "            {'address': '271 Springer Street',\n",
       "             'agenda': 'medicine',\n",
       "             'date': '1st',\n",
       "             'distance': 'best possible route',\n",
       "             'event': 'Swimming activity',\n",
       "             'location': 'Seattle',\n",
       "             'party': 'Ana',\n",
       "             'poi': \"chef chu's\",\n",
       "             'poi_type': 'coffee ot tea place',\n",
       "             'room': 'conference room 100',\n",
       "             'time': '2pm',\n",
       "             'traffic_info': 'avoid all heavy traffic',\n",
       "             'weather_attribute': '7 day forecast'})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "dic = set([])\n",
    "for d in dat:\n",
    "    for _d in d['dialogue']:\n",
    "        slots = _d['data'].get('slots')\n",
    "        if not slots: continue\n",
    "        dic.update([slots.get('distance')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dic = defaultdict(str)\n",
    "for d in dat:\n",
    "    for _d in d['dialogue']:\n",
    "        dic.update(_d.get('data',{}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "requested_slots = list(dic['requested'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['kb', 'task', 'uuid'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['scenario'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'address', 'event', 'time', 'room', 'party', 'poi_type', 'location', 'tuesday', 'thursday', 'friday', 'wednesday', 'traffic_info', 'date', 'agenda', 'saturday', 'poi', 'distance', 'sunday', 'monday', 'today'}\n"
     ]
    }
   ],
   "source": [
    "s=set([])\n",
    "for d in dat:\n",
    "    try:\n",
    "        keys=d['scenario']['kb']['column_names']\n",
    "    except:\n",
    "        continue\n",
    "    s.update(keys)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'car collision nearby', 'moderate traffic', 'road block nearby', 'heavy traffic', 'no traffic'}\n"
     ]
    }
   ],
   "source": [
    "s=set([])\n",
    "for d in dat:\n",
    "    try:\n",
    "        keys=[t['traffic_info'] for t in d['scenario']['kb']['items'] ]\n",
    "    except:\n",
    "        continue\n",
    "    s.update(keys)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weekly forecast', 'calendar', 'location information'}\n"
     ]
    }
   ],
   "source": [
    "s=set([])\n",
    "for d in dat:\n",
    "    try:\n",
    "        keys=[d['scenario']['kb']['kb_title']]\n",
    "    except:\n",
    "        continue\n",
    "    s.update(keys)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'schedule', 'weather', 'navigate'}\n"
     ]
    }
   ],
   "source": [
    "s=set([])\n",
    "for d in dat:\n",
    "    try:\n",
    "        keys=[d['scenario']['task']['intent']]\n",
    "    except:\n",
    "        print(d)\n",
    "        continue\n",
    "    s.update(keys)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'set'>, {'location information': {'navigate'}, 'calendar': {'schedule'}, 'weekly forecast': {'weather'}})\n"
     ]
    }
   ],
   "source": [
    "s=defaultdict(set)\n",
    "for d in dat:\n",
    "    try:\n",
    "        s[d['scenario']['kb']['kb_title']].update([d['scenario']['task']['intent']])\n",
    "    except:\n",
    "        print(d)\n",
    "        continue\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'set'>, {'navigate': {'address', 'poi_type', 'traffic_info', 'poi', 'distance'}, 'schedule': {'event', 'time', 'room', 'party', 'date', 'agenda'}, 'weather': {'location', 'thursday', 'tuesday', 'sunday', 'friday', 'wednesday', 'saturday', 'monday', 'today'}})\n"
     ]
    }
   ],
   "source": [
    "s=defaultdict(set)\n",
    "for d in dat:\n",
    "    try:\n",
    "        s[d['scenario']['task']['intent']].update(d['scenario']['kb']['column_names'])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dialogue': [{'turn': 'driver', 'data': {'end_dialogue': False, 'utterance': 'check forecast for new york'}}, {'turn': 'assistant', 'data': {'end_dialogue': False, 'requested': {'weather_attribute': True, 'date': False, 'location': True}, 'slots': {'location': ' new york'}, 'utterance': 'The weather this week will be hail and rain on Thursday and Sunday, with the lowest temperature being 20F and the highest 90F, Monday will be the only hot day. '}}, {'turn': 'driver', 'data': {'end_dialogue': False, 'utterance': 'Will it be overcast on friday?'}}, {'turn': 'assistant', 'data': {'end_dialogue': False, 'requested': {'weather_attribute': True, 'date': True, 'location': False}, 'slots': {'weather_attribute': 'overcast', 'date': 'Friday'}, 'utterance': 'On Friday it will be cloudy on New York'}}, {'turn': 'driver', 'data': {'end_dialogue': False, 'utterance': 'Thank you!'}}, {'turn': 'assistant', 'data': {'end_dialogue': True, 'requested': {'weather_attribute': False, 'date': False, 'location': False}, 'slots': {}, 'utterance': \"You're welcome!\"}}], 'scenario': {'kb': {'items': [{'monday': 'stormy, low of 50F, high of 70F', 'tuesday': 'cloudy, low of 20F, high of 40F', 'friday': 'windy, low of 20F, high of 40F', 'wednesday': 'clear skies, low of 50F, high of 60F', 'thursday': 'stormy, low of 50F, high of 70F', 'sunday': 'hail, low of 30F, high of 50F', 'location': 'manhattan', 'saturday': 'snow, low of 60F, high of 70F', 'today': 'monday'}, {'monday': 'overcast, low of 50F, high of 70F', 'tuesday': 'stormy, low of 90F, high of 100F', 'friday': 'clear skies, low of 70F, high of 80F', 'wednesday': 'frost, low of 50F, high of 60F', 'thursday': 'snow, low of 80F, high of 90F', 'sunday': 'dry, low of 70F, high of 80F', 'location': 'camarillo', 'saturday': 'blizzard, low of 40F, high of 50F', 'today': 'monday'}, {'monday': 'dry, low of 50F, high of 70F', 'tuesday': 'warm, low of 80F, high of 90F', 'friday': 'cloudy, low of 70F, high of 80F', 'wednesday': 'hail, low of 20F, high of 40F', 'thursday': 'blizzard, low of 40F, high of 60F', 'sunday': 'stormy, low of 90F, high of 100F', 'location': 'menlo park', 'saturday': 'foggy, low of 50F, high of 60F', 'today': 'monday'}, {'monday': 'hot, low of 70F, high of 90F', 'tuesday': 'foggy, low of 70F, high of 90F', 'friday': 'cloudy, low of 70F, high of 90F', 'wednesday': 'dry, low of 40F, high of 50F', 'thursday': 'hail, low of 40F, high of 50F', 'sunday': 'rain, low of 20F, high of 30F', 'location': 'new york', 'saturday': 'clear skies, low of 70F, high of 90F', 'today': 'monday'}, {'monday': 'windy, low of 70F, high of 80F', 'tuesday': 'overcast, low of 40F, high of 50F', 'friday': 'hail, low of 60F, high of 70F', 'wednesday': 'hail, low of 90F, high of 100F', 'thursday': 'rain, low of 80F, high of 100F', 'sunday': 'frost, low of 20F, high of 30F', 'location': 'corona', 'saturday': 'warm, low of 60F, high of 80F', 'today': 'monday'}, {'monday': 'cloudy, low of 90F, high of 100F', 'tuesday': 'dry, low of 30F, high of 40F', 'friday': 'foggy, low of 20F, high of 30F', 'wednesday': 'stormy, low of 80F, high of 100F', 'thursday': 'snow, low of 20F, high of 30F', 'sunday': 'drizzle, low of 50F, high of 60F', 'location': 'seattle', 'saturday': 'drizzle, low of 40F, high of 60F', 'today': 'monday'}, {'monday': 'drizzle, low of 70F, high of 80F', 'tuesday': 'overcast, low of 40F, high of 60F', 'friday': 'foggy, low of 50F, high of 60F', 'wednesday': 'overcast, low of 60F, high of 80F', 'thursday': 'snow, low of 20F, high of 40F', 'sunday': 'cloudy, low of 30F, high of 40F', 'location': 'grand rapids', 'saturday': 'dry, low of 90F, high of 100F', 'today': 'monday'}], 'column_names': ['location', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'today'], 'kb_title': 'weekly forecast'}, 'task': {'intent': 'weather'}, 'uuid': '89cae425-6ff7-48b4-8b42-ec50b541dc63'}}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "defaultdict(<class 'set'>, {})\n"
     ]
    }
   ],
   "source": [
    "s=defaultdict(set)\n",
    "for d in dat:\n",
    "    try:\n",
    "        if d['scenario']['task']['intent'] == 'weather':\n",
    "            print(d)\n",
    "            print('\\n\\n\\n\\n')\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KB results with beilef states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['distance',\n",
       " 'poi_type',\n",
       " 'traffic_info',\n",
       " 'poi',\n",
       " 'date',\n",
       " 'party',\n",
       " 'event',\n",
       " 'time',\n",
       " 'location',\n",
       " 'weather_attribute',\n",
       " 'room',\n",
       " 'address',\n",
       " 'agenda']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "belief_state_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'str'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defaultdict(lambda: 'str')['hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def sim(a, b):\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "\n",
    "def close(a,b):\n",
    "    return sim(a,b)>=0.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import dateutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_op(s1,op,s2):\n",
    "    m1=re.search(r'([0-9]+)',s1)\n",
    "    m2=re.search(r'([0-9]+)',s2)\n",
    "    if m1 is None or m2 is None:\n",
    "        if m1 is None:\n",
    "            try:\n",
    "                num1=dateutil.parser.parse(s1)\n",
    "            except ValueError:\n",
    "                return False\n",
    "        if m2 is None:\n",
    "            try:\n",
    "                num2=dateutil.parser.parse(s2)\n",
    "            except ValueError:\n",
    "                return False      \n",
    "    else:\n",
    "        num1=int(m1.group())\n",
    "        num2=int(m2.group())\n",
    "    try:\n",
    "        if op=='equal to':\n",
    "            return num1==num2\n",
    "        if op=='greater than':\n",
    "            return num1>num2\n",
    "        if op=='less than':\n",
    "            return num1<num2\n",
    "    except TypeError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kb_results(kb_data,kb_intent,columns,pred_kb_intent,belief_state,operation):\n",
    "    \"\"\"This function gets the kb_data, column names and intent for which the kb is received. \n",
    "    If intent identified by the bot is nto same as kb_intent no results will be returned.\n",
    "    \n",
    "    intents types: {'schedule', 'weather', 'navigate'}\n",
    "    \n",
    "    operation should have same keys as belief state with following possible values:\n",
    "    str, =, >, <, minimum, maximum  indexed from 0 to 5\n",
    "    Use None for all the values not numerical. If not None, operation would be performend\n",
    "    \"\"\"\n",
    "    #defaultdict(<class 'set'>, {'navigate': {'poi', 'distance', 'poi_type', 'traffic_info', 'address'},, 'weather': {'thursday', 'sunday', 'today', 'friday', 'wednesday', 'tuesday', 'saturday', 'location', 'monday'}})\n",
    "    if pred_kb_intent!=kb_intent:\n",
    "        return [],[]\n",
    "    if kb_data is None:\n",
    "        return [],[]\n",
    "    results = np.ones(len(kb_data))\n",
    "    confidence = np.ones(len(kb_data))\n",
    "    # column names possiblity: {'room', 'party', 'event', 'agenda', 'date', 'time'}  \n",
    "    # Note that date and time are immutable and non-comparable in current dialog, so they are treated as strings\n",
    "    col_types = defaultdict(lambda: 'str')\n",
    "    if any(k not in columns for k in belief_state.keys()):\n",
    "        return [],[]\n",
    "    for k in belief_state.keys():\n",
    "        min_idx = 0\n",
    "        min_val = float('Inf')\n",
    "        max_idx = 0\n",
    "        max_val = -float('Inf')\n",
    "        for i,items in enumerate(kb_data):\n",
    "            if results[i] == 0: continue\n",
    "            if col_types[k]=='str':\n",
    "                if not (belief_state.get(k) and items.get(k) and operation.get(k) is not None):\n",
    "                    results[i]=0\n",
    "                    confidence[i]=0\n",
    "                else:\n",
    "                    results[i]=0\n",
    "                    if operation[k]==0 and close(belief_state[k],items[k]):\n",
    "                        # Doing string comparison\n",
    "                        results[i]=1\n",
    "                        confidence[i] = confidence[i]*sim(belief_state.get(k),items.get(k))\n",
    "                    elif operation[k]==1:\n",
    "                        #Doing equal comparison extracting the first number\n",
    "                        if num_op(belief_state[k],'equal to',items[k]):\n",
    "                            results[i]=1\n",
    "                    elif operation[k]==2:\n",
    "                        #Doing greater than comparison extracting the first number\n",
    "                        if num_op(belief_state[k],'less than',items[k]):\n",
    "                            results[i]=1\n",
    "                    elif operation[k]==3:\n",
    "                        #Doing less than comparison extracting the first number\n",
    "                        if num_op(belief_state[k],'greater than',items[k]):\n",
    "                            results[i]=1\n",
    "                    elif operation[k]==4:\n",
    "                        #Doing mimum comparison extracting the first number\n",
    "                        res,val = num_op(belief_state[k],'less than',str(min_val))\n",
    "                        if res:\n",
    "                            results[i]=1\n",
    "                            results[min_idx] = 0\n",
    "                            confidence[min_idx] = 0\n",
    "                            min_val = val\n",
    "                            min_idx = i\n",
    "                    elif operation[k]==5:\n",
    "                        #Doing maximum comparison extracting the first number\n",
    "                        res,val = num_op(belief_state[k],'greater than',str(min_val))\n",
    "                        if res:\n",
    "                            results[i]=1\n",
    "                            results[max_idx] = 0\n",
    "                            confidence[max_idx] = 0\n",
    "                            max_val = val\n",
    "                            max_idx = i\n",
    "    \n",
    "           \n",
    "    return np.array(results),np.array(confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_data = dat[22]['scenario']['kb']['items']\n",
    "kb_intent = dat[22]['scenario']['task']['intent']\n",
    "columns = dat[22]['scenario']['kb']['column_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_kb_intent = kb_intent\n",
    "belief_state = {'traffic_info':'no traffic','distance':'6 miles'}\n",
    "operation = {'traffic_info':0,'distance':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 0., 0., 0., 1., 1., 0., 0.]),\n",
       " array([0.69230769, 1.        , 1.        , 1.        , 0.69230769,\n",
       "        1.        , 0.69565217, 1.        ]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb_results(kb_data,kb_intent,columns,pred_kb_intent,belief_state,operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'address': '271 Springer Street',\n",
       " 'distance': '1 miles',\n",
       " 'poi': 'Mandarin Roots',\n",
       " 'poi_type': 'chinese restaurant',\n",
       " 'traffic_info': 'no traffic'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb_data[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(t):\n",
    "    return word_tokenize(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2425/2425 [00:00<00:00, 73058.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# collection of kb documents\n",
    "doc_kb=[]\n",
    "for d in tqdm(dat):\n",
    "    try:\n",
    "        for item in d['scenario']['kb']['items']:\n",
    "            [doc_kb.append(t)  for t in item.values()]\n",
    "    except TypeError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2425/2425 [00:00<00:00, 210348.41it/s]\n"
     ]
    }
   ],
   "source": [
    "doc_colnames=[]\n",
    "for d in tqdm(dat):\n",
    "    try:\n",
    "        item = d['scenario']['kb']['column_names']\n",
    "        [doc_colnames.append(t)  for t in item]\n",
    "    except TypeError:\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns=list(set(doc_colnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary for the databases\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer=Tokenizer(filters=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(doc_kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts([\"<SOS>\",\"<EOS>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the db model first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns_wi={a:i for i,a in enumerate(all_columns)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all_columns\n",
    "assert tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_QUERIES = 1\n",
    "NUM_COL = len(all_columns)\n",
    "DB_VOCAB_LEN = len(tokenizer.word_index)+1\n",
    "THRESHOLD = 0.5\n",
    "MAX_DB_RESULTS = 5\n",
    "MAX_ENTITY_LENGTH = 10\n",
    "OPERATOR_LEN = 6\n",
    "NUM_INTENTS = 3\n",
    "EMBEDDING_SIZE=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.zeros((5,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting rules based db to desired output first\n",
    "def results_to_vector(bs_output,pred_intent,operation,kb_data,kb_intent):\n",
    "    assert bs_output.shape == (NUM_COL,MAX_ENTITY_LENGTH,DB_VOCAB_LEN)\n",
    "    assert operation.shape == (NUM_COL,OPERATOR_LEN)\n",
    "    pred_intent = np.argmax(pred_intent) if max(pred_intent)>THRESHOLD else None\n",
    "    kb_intent = np.argmax(kb_intent)\n",
    "    output=np.zeros((MAX_DB_RESULTS,NUM_COL,MAX_ENTITY_LENGTH,DB_VOCAB_LEN))\n",
    "    if intent is None:\n",
    "        return output\n",
    "    q=bs_output\n",
    "    op = operation\n",
    "    op_conf =  np.max(op,axis=-1)\n",
    "    op_classes = np.argmax(op,axis=-1) \n",
    "    op_classes = [_q if _q_conf>THRESHOLD else None for _q,_q_conf in zip(op_classes,op_conf)]\n",
    "\n",
    "    q_ents = np.argmax(q,axis=-1)\n",
    "    q_confs = np.max(q,axis=-1)\n",
    "    q_mask = np.array(q_confs>THRESHOLD,dtype='float32')\n",
    "    q_ents = q_mask*q_ents\n",
    "    q_words = [\" \".join([tokenizer.index_word[_q] for _q in __q if _q!=0]) for __q in q_ents]\n",
    "    # Now that q_words and op_classes are known\n",
    "    bs={}\n",
    "    operations = {}\n",
    "    for j,ent in enumerate(q_words):\n",
    "        if ent is None or ent==\"\": continue\n",
    "        bs[all_columns[j]]=ent\n",
    "        operations[all_columns[j]] = op_classes[j]\n",
    "    result,confidence = kb_results(kb_data,kb_intent,columns,pred_intent,bs,operations)\n",
    "    result=np.array(result)\n",
    "    confidence=np.array(confidence)\n",
    "    result = result[np.argsort(confidence)[-1::-1]]\n",
    "    confidence = confidence[np.argsort(confidence)[-1::-1]]\n",
    "    final_result=[kb_data[_i] for _i,(c,r) in enumerate(zip(confidence,result)) if c>=THRESHOLD and r==1]\n",
    "    confidence=[confidence[_i] for _i,(c,r) in enumerate(zip(confidence,result)) if c>=THRESHOLD and r==1]\n",
    "    kb_result = np.zeros((MAX_DB_RESULTS,NUM_COL,MAX_ENTITY_LENGTH,DB_VOCAB_LEN))\n",
    "    for j,r in enumerate(final_result):\n",
    "        if j==MAX_DB_RESULTS: break\n",
    "        for k,v in r.items():\n",
    "            kb_result[j,all_columns_wi[k]] = to_categorical(pad_sequences(tokenizer.texts_to_sequences([v]),maxlen=MAX_ENTITY_LENGTH)\\\n",
    "                                                             ,num_classes=DB_VOCAB_LEN)*confidence[j]\n",
    "\n",
    "    output = kb_result\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_output=np.zeros((NUM_COL,MAX_ENTITY_LENGTH,DB_VOCAB_LEN))\n",
    "intent=np.array([1,0,0])\n",
    "operation = np.zeros((NUM_COL,OPERATOR_LEN))\n",
    "bs_output[19,0]=to_categorical(tokenizer.texts_to_sequences(['dish'])[0],num_classes=DB_VOCAB_LEN)\n",
    "operation[19,0]=1.0\n",
    "kb_data = dat[0]['scenario']['kb']['items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input,Dense,LSTM,Embedding,TimeDistributed, RepeatVector, Concatenate,Reshape\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1, 20, 50)\n"
     ]
    }
   ],
   "source": [
    "# model for db\n",
    "bs_input = Input(shape=(MAX_QUERIES,NUM_COL,MAX_ENTITY_LENGTH,DB_VOCAB_LEN))\n",
    "intent_input = Input(shape=(MAX_QUERIES,NUM_INTENTS,))\n",
    "operation_input = Input(shape=(MAX_QUERIES,NUM_COL,OPERATOR_LEN))\n",
    "\n",
    "LSTM_bs_emb = TimeDistributed(TimeDistributed(LSTM(50,return_sequences=False,return_state=False)))(bs_input)\n",
    "rep_intent_input = TimeDistributed(RepeatVector(NUM_COL))(intent_input)\n",
    "print(LSTM_bs_emb.shape)\n",
    "all_steps = Concatenate(axis=-1)([LSTM_bs_emb,operation_input,rep_intent_input])\n",
    "all_steps = Lambda(lambda x: tf.reshape(x,shape=(-1,MAX_QUERIES,NUM_COL*(50+OPERATOR_LEN+NUM_INTENTS))))(all_steps)\n",
    "encoder_lstm = Dense(50,activation='relu')(all_steps)\n",
    "encoder_lstm = TimeDistributed(RepeatVector(MAX_DB_RESULTS))(encoder_lstm)\n",
    "\n",
    "decoder_lstm1 = TimeDistributed(LSTM(50,return_sequences=True))(encoder_lstm)\n",
    "\n",
    "decoder_lstm1 = Dense(NUM_COL*50,activation='relu')(decoder_lstm1)\n",
    "decoder_lstm1 = Lambda(lambda x: tf.reshape(x,shape=(-1,MAX_QUERIES,MAX_DB_RESULTS,NUM_COL,50)))(decoder_lstm1)\n",
    "\n",
    "\n",
    "decoder_lstm2 = TimeDistributed(Lambda(lambda x: K.tile(K.expand_dims(x,axis=-2),[1,1,1,MAX_ENTITY_LENGTH,1])))(decoder_lstm1)\n",
    "decoder_lstm3 = TimeDistributed(TimeDistributed(TimeDistributed(LSTM(50,return_sequences=True))))(decoder_lstm2)\n",
    "\n",
    "out = Dense(DB_VOCAB_LEN,activation='softmax')(decoder_lstm3)\n",
    "db_model = Model(inputs=[bs_input,intent_input,operation_input],outputs=[out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_94 (InputLayer)           (None, 1, 20, 10, 38 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_95 (InputLayer)           (None, 1, 3)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_66 (TimeDistri (None, 1, 20, 50)    88000       input_94[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_96 (InputLayer)           (None, 1, 20, 6)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_67 (TimeDistri (None, 1, 20, 3)     0           input_95[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1, 20, 59)    0           time_distributed_66[0][0]        \n",
      "                                                                 input_96[0][0]                   \n",
      "                                                                 time_distributed_67[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_97 (Lambda)              (None, 1, 1180)      0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_72 (Dense)                (None, 1, 50)        59050       lambda_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_68 (TimeDistri (None, 1, 5, 50)     0           dense_72[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_69 (TimeDistri (None, 1, 5, 50)     20200       time_distributed_68[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_73 (Dense)                (None, 1, 5, 1000)   51000       time_distributed_69[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_98 (Lambda)              (None, 1, 5, 20, 50) 0           dense_73[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_70 (TimeDistri (None, 1, 5, 20, 10, 0           lambda_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_73 (TimeDistri (None, 1, 5, 20, 10, 20200       time_distributed_70[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_74 (Dense)                (None, 1, 5, 20, 10, 19839       time_distributed_73[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 258,289\n",
      "Trainable params: 258,289\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "db_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_model.compile(optimizer='adam',loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(w, t = 1.0):\n",
    "    e = np.exp(np.array(w) / t)\n",
    "    dist = e / np.sum(e)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = ['schedule', 'weather', 'navigate']\n",
    "def input_generator(batch_size,data=dat):\n",
    "    batch_data1=[]\n",
    "    batch_data2=[]\n",
    "    batch_data3=[]\n",
    "    target=[]\n",
    "    random_dat = [data[i] for i in np.random.permutation(len(data))]\n",
    "    ij=0\n",
    "    while True:\n",
    "        ij+=1\n",
    "        for d in random_dat:\n",
    "            kb_intent = d['scenario']['task']['intent']\n",
    "            kb_col_names = d['scenario']['kb']['column_names']\n",
    "            kb_data = d['scenario']['kb']['items']\n",
    "            true_vec_intent = np.zeros(NUM_INTENTS)\n",
    "            true_vec_intent[intents.index(kb_intent)]=1.0\n",
    "            pred_intent = softmax(np.random.normal(size=NUM_INTENTS,loc=100,scale=5))\n",
    "            bs_input = np.zeros((NUM_COL,MAX_ENTITY_LENGTH,DB_VOCAB_LEN))\n",
    "            operation = np.zeros((NUM_COL,OPERATOR_LEN))\n",
    "            num_cols_to_have = np.random.randint(NUM_COL)\n",
    "            num_ents_to_have = [np.random.randint(NUM_COL) for _ in range(num_cols_to_have)]\n",
    "            for ii in range(num_cols_to_have):\n",
    "                for j in range(num_ents_to_have[ii]):\n",
    "                    ix=(np.random.randint(NUM_COL),np.random.randint(MAX_ENTITY_LENGTH))\n",
    "                    bs_input[ix] = softmax(np.random.normal(size=DB_VOCAB_LEN,loc=100,scale=5))\n",
    "                operation[np.random.randint(NUM_COL)] = softmax(np.random.normal(size=OPERATOR_LEN,loc=100,scale=5))\n",
    "            batch_data1.append(bs_input)\n",
    "            batch_data2.append(operation)\n",
    "            batch_data3.append(pred_intent)\n",
    "            target.append(results_to_vector(bs_input,pred_intent,operation,kb_data,true_vec_intent))\n",
    "            \n",
    "            if len(batch_data1)==batch_size:\n",
    "                yield [np.array(batch_data1),np.array(batch_data3),np.array(batch_data2)],np.array(target)\n",
    "                batch_data1=[]\n",
    "                batch_data2=[]\n",
    "                batch_data3=[]\n",
    "                target=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20, 10, 389)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(input_generator(1,dat))[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psutil\n",
      "  Downloading https://files.pythonhosted.org/packages/7d/9a/1e93d41708f8ed2b564395edfa3389f0fd6d567597401c2e5e2775118d8b/psutil-5.4.7.tar.gz (420kB)\n",
      "\u001b[K    100% |████████████████████████████████| 430kB 323kB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: psutil\n",
      "  Running setup.py bdist_wheel for psutil ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/arusia/.cache/pip/wheels/e2/9d/ea/1913d16f19bb927c32197308dec69cd8d10b61be8f7e265524\n",
      "Successfully built psutil\n",
      "Installing collected packages: psutil\n",
      "Successfully installed psutil-5.4.7\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'psutil'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-6eb0e540343b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpsutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmemory_exhausted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmemory_percent_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Memory Exhausted\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'psutil'"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "def memory_exhausted():\n",
    "    if memory_percent_available()<=10:\n",
    "        print(\"Memory Exhausted\")\n",
    "        exit()\n",
    "class memCall(keras.callbacks.Callback):\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        memory_exhausted()\n",
    "        \n",
    "def memory_percent_available():\n",
    "    return psutil.virtual_memory().available/psutil.virtual_memory().total*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint=keras.callbacks.ModelCheckpoint('./db_model.h5',save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "db_model.fit_generator(input_generator(batch_size),validation_data=input_generator(batch_size,valid_dat),steps_per_epoch=1250,epochs=10,validation_steps=10000,callbacks=[memCall(),checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the complete model now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model would receive the belief states and give out the sentences in the coeherent form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 30\n",
    "LATENT_DIM = 50\n",
    "CONV_VOCAB_LEN = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine.network import Network\n",
    "def get_model(db_m):\n",
    "    frozen_db_m = Network(db_m.input,db_m.output,name='frozen_db_model')\n",
    "    \n",
    "    text_in = Input(shape=(MAX_SEQ_LEN,EMBEDDING_SIZE))\n",
    "    text_tok_in = Input(shape=(MAX_SEQ_LEN,CONV_VOCAB_LEN))\n",
    "    emb_state_in_h = Input(shape=(LATENT_DIM,))\n",
    "    emb_state_in_c = Input(shape=(LATENT_DIM,))\n",
    "    \n",
    "    encoding,emb_state_out_h,emb_state_out_c = LSTM(LATENT_DIM,\\\n",
    "                                                    return_state=True)(text_in,\\\n",
    "                                            initial_state=[emb_state_in_h,emb_state_in_c])\n",
    "    \n",
    "    \n",
    "    \n",
    "    db_decoder_1 = Lambda(lambda x: K.tile(K.expand_dims(x,axis=-2),[1,MAX_QUERIES,1]))(encoding)\n",
    "    db_decoder_2 = LSTM(NUM_COL*LATENT_DIM,return_sequences=True)(db_decoder_1)\n",
    "    db_decoder_2 = Lambda(lambda x: tf.reshape(x,[-1,MAX_QUERIES,NUM_COL,LATENT_DIM]))(db_decoder_2)\n",
    "    \n",
    "    db_decoder_3 = Lambda(lambda x: K.tile(K.expand_dims(x,axis=-2),[1,1,1,MAX_ENTITY_LENGTH,1]))(db_decoder_2)\n",
    "    db = TimeDistributed(TimeDistributed(LSTM(LATENT_DIM,return_sequences=True)))(db_decoder_3)\n",
    "    bs_out = Dense(MAX_SEQ_LEN,activation='softmax')(db)\n",
    "    text_tok_in_reshape = Lambda(lambda x: tf.reshape(x,[-1,1,1,1,MAX_SEQ_LEN,CONV_VOCAB_LEN]))(text_tok_in)\n",
    "    \n",
    "    bs_out = Lambda(lambda x: K.expand_dims(x))(bs_out)\n",
    "    print(bs_out.shape,text_tok_in_reshape.shape)\n",
    "    bs_out = Lambda(lambda x: x[0]*x[1])([bs_out,text_tok_in])\n",
    "    print(bs_out.shape)\n",
    "    op_out = Dense(LATENT_DIM,activation='relu')(db_decoder_2)\n",
    "    op_out = Dense(OPERATOR_LEN,activation='softmax')(op_out)\n",
    "    print(op_out.shape)\n",
    "    intent_out = Dense(LATENT_DIM,activation='relu')(encoding)\n",
    "    intent_out = Dense(NUM_INTENTS,activation='softmax')(intent_out)\n",
    "    intent_out = RepeatVector(MAX_QUERIES)(intent_out)\n",
    "    model_bs = Model(inputs=[text_in,emb_state_in_h,emb_state_in_c,text_tok_in],outputs=[bs_out,intent_out,op_out])\n",
    "    # DECODER\n",
    "    decoder_input_db = frozen_db_m([bs_out,intent_out,op_out])\n",
    "    other_bs_input = encoding\n",
    "    \n",
    "    #Mdecoder_input_b is of length None,MAX_QUERIES,MAX_DB_RESULTS,NUM_COL,MAX_ENTITY_LENGTH,DB_VOCAB_LEN\n",
    "    dbout_encoder_maxent = TimeDistributed(TimeDistributed(TimeDistributed(LSTM(LATENT_DIM))))(decoder_input_db)\n",
    "    dbout_encoder_ncol =  Lambda(lambda x: tf.reshape(x,[-1,MAX_QUERIES,MAX_DB_RESULTS,NUM_COL*LATENT_DIM]))(dbout_encoder_maxent)\n",
    "    dbout_encoder_ncol = Dense(LATENT_DIM,activation='relu')(dbout_encoder_ncol)\n",
    "    dbout_encoder_dbr = TimeDistributed(LSTM(LATENT_DIM))(dbout_encoder_ncol)\n",
    "    dbout_encoder = Concatenate(axis=-1)([dbout_encoder_dbr,other_bs_input])\n",
    "    dbout_encoder,dbout_s,dbout_h = LSTM(LATENT_DIM,return_state=True)(dbout_encoder)\n",
    "    decoder_hidden_inputs = [dbout_s,dbout_h]\n",
    "    \n",
    "    decoder_inputs = Input(shape=(None,MAX_SEQ_LEN,EMBEDDING_SIZE))\n",
    "    \n",
    "    decoder_LSTM = LSTM(LATENT_DIM,return_sequences=True,return_state=True)\n",
    "    decoder_outputs,_,_ = decoder_LSTM(decoder_inputs_conc,intial_state=decoder_hidden_inputs)\n",
    "    decoder_outputs = Dense(CONV_VOCAB_LEN,activation='softmax')(decoder_outputs)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1, 20, 10, 30, 1) (?, 1, 1, 1, 30, 30000)\n",
      "(?, 1, 20, 10, 30, 30000)\n",
      "(?, 1, 20, 6)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate tuple (not \"list\") to tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-5174b78116b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-110-ce25604b2b33>\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(db_m)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mmodel_bs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memb_state_in_h\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memb_state_in_c\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext_tok_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbs_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mintent_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mop_out\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# DECODER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mdecoder_input_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrozen_db_m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbs_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mintent_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mop_out\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mother_bs_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m                                          \u001b[0;34m'You can build it manually via: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[0;32m--> 431\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/layers/wrappers.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mchild_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_input_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate tuple (not \"list\") to tuple"
     ]
    }
   ],
   "source": [
    "get_model(db_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input((10,vocab_len))\n",
    "encoder = LSTM(50,return_state=True)\n",
    "out,out_h,out_c=encoder(inp)\n",
    "out = Dense(vocab_len,activation='softmax')(out)\n",
    "model_rnn_2=Model(inputs=[inp],outputs=out)\n",
    "\n",
    "out_3 = frozen_db_model(model_rnn_2.output)\n",
    "out_35 = RepeatVector(10)(out_3)\n",
    "decoder = LSTM(50,return_sequences=True,return_state=True)\n",
    "out_4,_,_= decoder(out_35,initial_state=[out_h,out_c])\n",
    "out_4 = Dense(vocab_len,activation='softmax')(out_4)\n",
    "model_4 = Model(inputs=[model_rnn_2.input],outputs=[model_rnn_2.output,out_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
